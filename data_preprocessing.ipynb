{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Used"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ffmpeg-python\n",
    "* av\n",
    "* cmake\n",
    "* dlib  (based on the python version)\n",
    "* face-recognition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports & Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import face_recognition\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_ORG = './dataset_original/'\n",
    "DS_IFRAME = './dataset_IFrames/'\n",
    "DS_FACE = './dataset_face/'\n",
    "DS_FINAL = './dataset_final/'\n",
    "DS_SEG = './dataset_segments/'\n",
    "\n",
    "CELEB_REAL = 'Celeb-real/'\n",
    "CELEB_FAKE = 'Celeb-synthesis/'\n",
    "YT_REAL = 'YouTube-real/'\n",
    "\n",
    "SEG = ['seg_1_', 'seg_2_', 'seg_3_']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I-Frame Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vid = av.open('dataset_original/Celeb-real/id0_0000.mp4')\n",
    "\n",
    "for packet in test_vid.demux():\n",
    "    for frame in packet.decode():\n",
    "        print(f'{frame.pict_type} - {frame.key_frame}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = av.open('dataset_original/Celeb-real/id0_0000.mp4')\n",
    "test_output = av.open('dataset_IFrames/id0_0000.mp4', 'w')\n",
    "\n",
    "in_stream = test_input.streams.video[0]\n",
    "in_stream.codec_context.skip_frame = \"NONKEY\"\n",
    "\n",
    "out_stream = test_output.add_stream(template=in_stream)\n",
    "\n",
    "for packet in test_input.demux(in_stream):\n",
    "    if packet.dts is None:\n",
    "        continue\n",
    "\n",
    "    if packet.is_keyframe:\n",
    "        packet.stream = out_stream\n",
    "        test_output.mux(packet)\n",
    "\n",
    "test_input.close()\n",
    "test_output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for video in os.listdir(DS_ORG + CELEB_REAL):\n",
    "    count += 1\n",
    "    if (count == 10):\n",
    "        break\n",
    "\n",
    "    input_vid = av.open(DS_ORG + CELEB_REAL + video)\n",
    "    output_vid = av.open(DS_IFRAME + CELEB_REAL + video, 'w')\n",
    "\n",
    "    in_stream = input_vid.streams.video[0]\n",
    "    in_stream.codec_context.skip_frame = \"NONKEY\"\n",
    "\n",
    "    out_stream = output_vid.add_stream(template=in_stream)\n",
    "\n",
    "    for packet in input_vid.demux(in_stream):\n",
    "        if packet.dts is None:\n",
    "            continue\n",
    "    \n",
    "        if packet.is_keyframe:\n",
    "            packet.stream = out_stream\n",
    "            output_vid.mux(packet)\n",
    "\n",
    "    input_vid.close()\n",
    "    output_vid.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(src_dir, dest_dir, vid_class, filename):\n",
    "    input_vid = av.open(src_dir + vid_class + filename)\n",
    "    output_vid = av.open(dest_dir + vid_class + filename, 'w')\n",
    "\n",
    "    in_stream = input_vid.streams.video[0]\n",
    "    in_stream.codec_context.skip_frame = \"NONKEY\"\n",
    "\n",
    "    out_stream = output_vid.add_stream(template=in_stream)\n",
    "\n",
    "    for packet in input_vid.demux(in_stream):\n",
    "        if packet.dts is None:\n",
    "            continue\n",
    "\n",
    "        if packet.is_keyframe:\n",
    "            packet.stream = out_stream\n",
    "            output_vid.mux(packet)\n",
    "\n",
    "    input_vid.close()\n",
    "    output_vid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting I-Frames from real celebrity videos\n",
    "\n",
    "for video in os.listdir(DS_ORG + CELEB_REAL):\n",
    "    extract_frames(DS_ORG, DS_IFRAME, CELEB_REAL, video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting I-Frames from real youtube videos\n",
    "\n",
    "for video in os.listdir(DS_ORG + YT_REAL):\n",
    "    extract_frames(DS_ORG, DS_IFRAME, YT_REAL, video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting I-Frames from deepfake celebrity videos\n",
    "# 408 vidoes chosen at random to ensure equal amount of real and fake vidoes \n",
    "# used for training. (158 real celeb videos + 250 real youtube vidoes)\n",
    "\n",
    "video_list = random.sample(os.listdir(DS_ORG + CELEB_FAKE), 408)\n",
    "for video in video_list:\n",
    "    extract_frames(DS_ORG, DS_IFRAME, CELEB_FAKE, video)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MesoNet works best with images having 256x256 dimension\n",
    "# If face location borders span a smaller distance, extend the borders\n",
    "# on either side equally to ensure 256x256 image\n",
    "\n",
    "def normalize_face_borders(low, high):\n",
    "    diff = high - low\n",
    "    if diff >= 256:\n",
    "        return\n",
    "\n",
    "    x = diff / 2\n",
    "    if (low >= x): \n",
    "        low -= x\n",
    "    else:\n",
    "        x = x + (x - low) + (1 if diff % 2 == 1 else 0)\n",
    "        low = 0\n",
    "\n",
    "    high += x\n",
    "\n",
    "    return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New normalize function to always make the cropped face image 256x256 dimension\n",
    "# which will be fed as input to the MesoNet\n",
    "\n",
    "def modified_normalize_face_borders(low, high, boundary):\n",
    "    diff = high - low\n",
    "\n",
    "    if diff <= 256:\n",
    "        offset = 256 - diff\n",
    "        low = max(0, min(low - offset / 2 , low))\n",
    "        high = min(boundary, max(high + (offset - offset / 2), high))\n",
    "\n",
    "    return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crop_window(face_location, height, width):\n",
    "    face_location = (face_location[0][3], height - face_location[0][0], face_location[0][1], height - face_location[0][2])\n",
    "\n",
    "    left, right = modified_normalize_face_borders(face_location[0], face_location[2], width)\n",
    "    bot, top = modified_normalize_face_borders(face_location[3], face_location[1], height)\n",
    "\n",
    "    face_location = (left, height - top, right, height - bot)\n",
    "\n",
    "    return face_location"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = av.open('dataset_IFrames/Celeb-real/id0_0000.mp4')\n",
    "\n",
    "count = 0\n",
    "\n",
    "for frame in test_input.decode():\n",
    "    nd_frame = frame.to_ndarray()\n",
    "    img_frame = frame.to_image()\n",
    "\n",
    "    height, width = img_frame.height, img_frame.width\n",
    "\n",
    "    # Face location returned by face_recognition api: [(top, right, bottom, left)] in css terms\n",
    "    # Face location required by PIL.Image: (left, top, right, bottom)\n",
    "    face_location = face_recognition.api.face_locations(nd_frame)\n",
    "    face_location = get_crop_window(face_location, height, width)\n",
    "    \n",
    "    img_frame = img_frame.crop(face_location)\n",
    "    img_frame.save(f'dataset_face/Celeb-real/id0_0000_{count}.jpg')\n",
    "\n",
    "    count += 1 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turning Cropped faces to a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bitrate tolerance 128000 too small for bitrate 1024000, overriding\n"
     ]
    }
   ],
   "source": [
    "test_input = av.open('dataset_IFrames/Celeb-real/id10_0001.mp4')\n",
    "test_output = av.open('dataset_Face/Celeb-real/id10_0001.mp4', 'w')\n",
    "\n",
    "in_stream = test_input.streams.video[0]\n",
    "codec_name = in_stream.codec_context.name\n",
    "\n",
    "out_stream = test_output.add_stream(codec_name, 2)\n",
    "out_stream.width = in_stream.codec_context.width\n",
    "out_stream.height = in_stream.codec_context.height\n",
    "out_stream.pix_fmt = in_stream.codec_context.pix_fmt\n",
    "\n",
    "for frame in test_input.decode(in_stream):\n",
    "    img_frame = frame.to_image()\n",
    "    nd_frame = frame.to_ndarray()\n",
    "\n",
    "    height, width = img_frame.height, img_frame.width\n",
    "\n",
    "    # Face location returned by face_recognition api: [(top, right, bottom, left)]\n",
    "    # Face location required by PIL.Image: (left, top, right, bottom)\n",
    "    face_location = face_recognition.api.face_locations(nd_frame)\n",
    "    \n",
    "    if len(face_location) == 0:\n",
    "        continue\n",
    "\n",
    "    # face_location = (face_location[0][3], face_location[0][0], face_location[0][1], face_location[0][2])\n",
    "\n",
    "    # left, right = normalize_face_borders(face_location[0], face_location[2])\n",
    "    # bot, top = normalize_face_borders(face_location[3], face_location[1])\n",
    "    # face_location = (left, top, right, bot)\n",
    "\n",
    "    face_location = get_crop_window(face_location, height, width)\n",
    "    img_frame = img_frame.crop(face_location)\n",
    "\n",
    "    out_frame = av.VideoFrame.from_image(img_frame)\n",
    "    out_packet = out_stream.encode(out_frame)\n",
    "    test_output.mux(out_packet)\n",
    "\n",
    "out_packet = out_stream.encode(None)\n",
    "test_output.mux(out_packet)\n",
    "\n",
    "test_input.close()\n",
    "test_output.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cropped_faces_to_video(src_dir, dest_dir, vid_class, filename):\n",
    "    \n",
    "    input = av.open(src_dir + vid_class + filename)\n",
    "    output = av.open(dest_dir + vid_class + filename, 'w')\n",
    "\n",
    "    in_stream = input.streams.video[0]\n",
    "    codec_name = in_stream.codec_context.name\n",
    "\n",
    "    # output video dimension should be 256x256\n",
    "    out_stream = output.add_stream(codec_name, 2)\n",
    "    out_stream.width = 256\n",
    "    out_stream.height = 256\n",
    "    out_stream.pix_fmt = in_stream.codec_context.pix_fmt\n",
    "\n",
    "    for frame in input.decode(in_stream):\n",
    "        img_frame = frame.to_image()\n",
    "        nd_frame = frame.to_ndarray()\n",
    "\n",
    "        height, width = img_frame.height, img_frame.width\n",
    "\n",
    "        # Face location returned by face_recognition api: [(top, right, bottom, left)]\n",
    "        # Face location required by PIL.Image: (left, top, right, bottom)\n",
    "        face_location = face_recognition.api.face_locations(nd_frame)\n",
    "\n",
    "        # can't find a face, then skip that frame\n",
    "        # TODO : sync frame skipping with temporality stream\n",
    "        if len(face_location) == 0:\n",
    "            continue\n",
    "            \n",
    "        face_location = get_crop_window(face_location, height, width)\n",
    "        img_frame = img_frame.crop(face_location)\n",
    "        \n",
    "        out_frame = av.VideoFrame.from_image(img_frame)\n",
    "        out_packet = out_stream.encode(out_frame)\n",
    "        output.mux(out_packet)\n",
    "\n",
    "    out_packet = out_stream.encode(None)\n",
    "    output.mux(out_packet)\n",
    "\n",
    "    input.close()\n",
    "    output.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Method to save cropped faces to video\n",
    "\n",
    "The codec resizes the video according to specified dimension.\n",
    "The face_location from face_recognition api can be directly used without normalizing borders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_save_cropped_faces_to_video(src_dir, dest_dir, vid_class, filename):\n",
    "    \n",
    "    input = av.open(src_dir + vid_class + filename)\n",
    "    output = av.open(dest_dir + vid_class + filename, 'w')\n",
    "\n",
    "    in_stream = input.streams.video[0]\n",
    "    codec_name = in_stream.codec_context.name\n",
    "\n",
    "    # output video dimension should be 256x256\n",
    "    out_stream = output.add_stream(codec_name, 2)\n",
    "    out_stream.width = 256\n",
    "    out_stream.height = 256\n",
    "    out_stream.pix_fmt = in_stream.codec_context.pix_fmt\n",
    "\n",
    "    for frame in input.decode(in_stream):\n",
    "        img_frame = frame.to_image()\n",
    "        nd_frame = frame.to_ndarray()\n",
    "\n",
    "        height, width = img_frame.height, img_frame.width\n",
    "\n",
    "        # Face location returned by face_recognition api: [(top, right, bottom, left)]\n",
    "        # Face location required by PIL.Image: (left, top, right, bottom)\n",
    "        face_location = face_recognition.api.face_locations(nd_frame)\n",
    "\n",
    "        # can't find a face, then skip that frame\n",
    "        # TODO : sync frame skipping with temporality stream\n",
    "        if len(face_location) == 0:\n",
    "            continue\n",
    "        \n",
    "        # since the codec resizes the video depending on specified dimension\n",
    "        # no need to normalize borders\n",
    "        face_location = (face_location[0][3], face_location[0][0], face_location[0][1], face_location[0][2])\n",
    "        img_frame = img_frame.crop(face_location)\n",
    "        \n",
    "        out_frame = av.VideoFrame.from_image(img_frame)\n",
    "        out_packet = out_stream.encode(out_frame)\n",
    "        output.mux(out_packet)\n",
    "\n",
    "    out_packet = out_stream.encode(None)\n",
    "    output.mux(out_packet)\n",
    "\n",
    "    input.close()\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in os.listdir(DS_IFRAME + CELEB_REAL):\n",
    "    simple_save_cropped_faces_to_video(DS_IFRAME, DS_FACE, CELEB_REAL, video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in os.listdir(DS_IFRAME + CELEB_FAKE):\n",
    "    save_cropped_faces_to_video(DS_IFRAME, DS_FACE, CELEB_FAKE, video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in os.listdir(DS_IFRAME + YT_REAL):\n",
    "    save_cropped_faces_to_video(DS_IFRAME, DS_FACE, YT_REAL, video)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing videos into Segments of 3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment each video into 3 segments\n",
    "def segment_video(src_dir, dest_dir, vid_class, filename):\n",
    "    \n",
    "    input = av.open(src_dir + vid_class + filename)\n",
    "\n",
    "    in_stream = input.streams.video[0]\n",
    "    total_frames = in_stream.frames\n",
    "    \n",
    "    frames_per_segment = total_frames / 3\n",
    "\n",
    "    codec_name = in_stream.codec_context.name\n",
    "\n",
    "    count = 1\n",
    "    seg_no = 0\n",
    "\n",
    "    # output video dimension should be 224x224\n",
    "    output = av.open(dest_dir + vid_class + SEG[seg_no] + filename, 'w')\n",
    "    out_stream = output.add_stream(codec_name, 2)\n",
    "    out_stream.width = 224\n",
    "    out_stream.height = 224\n",
    "    out_stream.pix_fmt = in_stream.codec_context.pix_fmt\n",
    "\n",
    "    for frame in input.decode(in_stream):\n",
    "        img_frame = frame.to_image()\n",
    "        nd_frame = frame.to_ndarray()\n",
    "\n",
    "        height, width = img_frame.height, img_frame.width\n",
    "\n",
    "        if seg_no < 2 and count > frames_per_segment:\n",
    "            count = 1\n",
    "            seg_no += 1\n",
    "            out_packet = out_stream.encode(None)\n",
    "            output.mux(out_packet)\n",
    "            output.close()\n",
    "            output = av.open(dest_dir + vid_class + SEG[seg_no] + filename, 'w')\n",
    "            out_stream = output.add_stream(codec_name, 2)\n",
    "            out_stream.width = 224\n",
    "            out_stream.height = 224\n",
    "            out_stream.pix_fmt = in_stream.codec_context.pix_fmt\n",
    "\n",
    "        out_frame = av.VideoFrame.from_image(img_frame)\n",
    "        out_packet = out_stream.encode(out_frame)\n",
    "        output.mux(out_packet)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    out_packet = out_stream.encode(None)\n",
    "    output.mux(out_packet)\n",
    "    input.close()\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting 3-Segments from real celebrity videos\n",
    "\n",
    "for video in os.listdir(DS_ORG + CELEB_REAL):\n",
    "    segment_video(DS_ORG, DS_SEG, CELEB_REAL, video)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting 3-Segments from fake celebrity videos\n",
    "\n",
    "for video in os.listdir(DS_ORG + CELEB_REAL):\n",
    "    segment_video(DS_ORG, DS_SEG, CELEB_FAKE, video)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting 3-Segments from real youtube videos\n",
    "\n",
    "for video in os.listdir(DS_ORG + CELEB_REAL):\n",
    "    segment_video(DS_ORG, DS_SEG, YT_REAL, video)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoViAR (Compressed Video Action Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FINAL YEAR PROJECT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "881d28eb5c6ba00316dfeeffcae1bf362a8848d46bb33063d8b349af67e9b6e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
