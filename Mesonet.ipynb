{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq9_NfM01bAv"
      },
      "source": [
        "# Libraries Used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE15KxFc1bA7"
      },
      "source": [
        "* ffmpeg-python\n",
        "* av\n",
        "* cmake\n",
        "* dlib  (based on the python version)\n",
        "* face-recognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount = True)\n",
        "\n",
        "# !unzip drive/MyDrive/Deepfake-Detection.zip\n",
        "# %cd Deepfake-Detection\n",
        "\n",
        "!pip install ffmpeg-python\n",
        "!pip install av\n",
        "!pip install cmake\n",
        "!pip install dlib\n",
        "!pip install face-recognition"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY1acu8nB5-9",
        "outputId": "3a6630ae-4fd4-48d8-a7b3-af085efa2537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from ffmpeg-python) (0.16.0)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting av\n",
            "  Downloading av-10.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-10.0.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.8/dist-packages (3.22.6)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: dlib in /usr/local/lib/python3.8/dist-packages (19.24.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting face-recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting face-recognition-models>=0.3.0\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.8/dist-packages (from face-recognition) (19.24.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from face-recognition) (1.22.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from face-recognition) (7.1.2)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.8/dist-packages (from face-recognition) (7.1.2)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566185 sha256=16c7b55a9bb69a10e4aa2a96559ada550d8e85f86ff57b9fc0669afc5358df56\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/4b/8f/751e99d45f089bdf366a7d3e5066db3c2b84a62e4377f534d7\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face-recognition\n",
            "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVQ3hrMI1bA8"
      },
      "source": [
        "# Imports & Globals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCybHdKL1bA9"
      },
      "outputs": [],
      "source": [
        "import av\n",
        "import face_recognition\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkNZmcDM1bA_"
      },
      "outputs": [],
      "source": [
        "DS_ORG = './dataset_original/'\n",
        "DS_IFRAME = './dataset_IFrames/'\n",
        "DS_FACE = './dataset_face/'\n",
        "DS_FINAL = './dataset_final/'\n",
        "\n",
        "CELEB_REAL = 'Celeb-real/'\n",
        "CELEB_FAKE = 'Celeb-synthesis/'\n",
        "YT_REAL = 'YouTube-real/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxNZoBoP1bBA"
      },
      "source": [
        "# I-Frame Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdE0iepD1bBB"
      },
      "source": [
        "## Testing Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMEW9YUZ1bBC"
      },
      "outputs": [],
      "source": [
        "test_vid = av.open('dataset_original/Celeb-real/id0_0000.mp4')\n",
        "\n",
        "for packet in test_vid.demux():\n",
        "    for frame in packet.decode():\n",
        "        print(f'{frame.pict_type} - {frame.key_frame}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjMv2aZL1bBF"
      },
      "outputs": [],
      "source": [
        "test_input = av.open('dataset_original/Celeb-real/id0_0000.mp4')\n",
        "test_output = av.open('dataset_IFrames/id0_0000.mp4', 'w')\n",
        "\n",
        "in_stream = test_input.streams.video[0]\n",
        "in_stream.codec_context.skip_frame = \"NONKEY\"\n",
        "\n",
        "out_stream = test_output.add_stream(template=in_stream)\n",
        "\n",
        "for packet in test_input.demux(in_stream):\n",
        "    if packet.dts is None:\n",
        "        continue\n",
        "\n",
        "    if packet.is_keyframe:\n",
        "        packet.stream = out_stream\n",
        "        test_output.mux(packet)\n",
        "\n",
        "test_input.close()\n",
        "test_output.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lC23_3Ke1bBI"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for video in os.listdir(DS_ORG + CELEB_REAL):\n",
        "    count += 1\n",
        "    if (count == 10):\n",
        "        break\n",
        "\n",
        "    input_vid = av.open(DS_ORG + CELEB_REAL + video)\n",
        "    output_vid = av.open(DS_IFRAME + CELEB_REAL + video, 'w')\n",
        "\n",
        "    in_stream = input_vid.streams.video[0]\n",
        "    in_stream.codec_context.skip_frame = \"NONKEY\"\n",
        "\n",
        "    out_stream = output_vid.add_stream(template=in_stream)\n",
        "\n",
        "    for packet in input_vid.demux(in_stream):\n",
        "        if packet.dts is None:\n",
        "            continue\n",
        "    \n",
        "        if packet.is_keyframe:\n",
        "            packet.stream = out_stream\n",
        "            output_vid.mux(packet)\n",
        "\n",
        "    input_vid.close()\n",
        "    output_vid.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewqYb7Gy1bBK"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYcggV2D1bBL"
      },
      "outputs": [],
      "source": [
        "def extract_frames(src_dir, dest_dir, vid_class, filename):\n",
        "    input_vid = av.open(src_dir + vid_class + filename)\n",
        "    output_vid = av.open(dest_dir + vid_class + filename, 'w')\n",
        "\n",
        "    in_stream = input_vid.streams.video[0]\n",
        "    in_stream.codec_context.skip_frame = \"NONKEY\"\n",
        "\n",
        "    out_stream = output_vid.add_stream(template=in_stream)\n",
        "\n",
        "    for packet in input_vid.demux(in_stream):\n",
        "        if packet.dts is None:\n",
        "            continue\n",
        "\n",
        "        if packet.is_keyframe:\n",
        "            packet.stream = out_stream\n",
        "            output_vid.mux(packet)\n",
        "\n",
        "    input_vid.close()\n",
        "    output_vid.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o69EJX5h1bBL"
      },
      "outputs": [],
      "source": [
        "# Extracting I-Frames from real celebrity videos\n",
        "\n",
        "for video in os.listdir(DS_ORG + CELEB_REAL):\n",
        "    extract_frames(DS_ORG, DS_IFRAME, CELEB_REAL, video)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting I-Frames from real youtube videos\n",
        "\n",
        "for video in os.listdir(DS_ORG + YT_REAL):\n",
        "    extract_frames(DS_ORG, DS_IFRAME, YT_REAL, video)"
      ],
      "metadata": {
        "id": "3sXd3UtpCKZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW0rXlw_1bBN"
      },
      "outputs": [],
      "source": [
        "# Extracting I-Frames from deepfake celebrity videos\n",
        "# 408 vidoes chosen at random to ensure equal amount of real and fake vidoes \n",
        "# used for training. (158 real celeb videos + 250 real youtube vidoes)\n",
        "\n",
        "video_list = random.sample(os.listdir(DS_ORG + CELEB_FAKE), 408)\n",
        "for video in video_list:\n",
        "    extract_frames(DS_ORG, DS_IFRAME, CELEB_FAKE, video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7BJFyYP1bBN"
      },
      "source": [
        "# Face Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCnJ6KpL1bBO"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeER8G4Y1bBO"
      },
      "outputs": [],
      "source": [
        "# MesoNet works best with images having 256x256 dimension\n",
        "# If face location borders span a smaller distance, extend the borders\n",
        "# on either side equally to ensure 256x256 image\n",
        "\n",
        "def normalize_face_borders(low, high):\n",
        "    diff = high - low\n",
        "    if diff >= 256:\n",
        "        return\n",
        "\n",
        "    x = diff / 2\n",
        "    if (low >= x): \n",
        "        low -= x\n",
        "    else:\n",
        "        x = x + (x - low) + (1 if diff % 2 == 1 else 0)\n",
        "        low = 0\n",
        "\n",
        "    high += x\n",
        "\n",
        "    return low, high"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFkdN51G1bBO"
      },
      "outputs": [],
      "source": [
        "# New normalize function to always make the cropped face image 256x256 dimension\n",
        "# which will be fed as input to the MesoNet\n",
        "\n",
        "def modified_normalize_face_borders(low, high, boundary):\n",
        "    diff = high - low\n",
        "\n",
        "    if diff <= 256:\n",
        "        offset = 256 - diff\n",
        "        low = max(0, min(low - offset / 2 , low))\n",
        "        high = min(boundary, max(high + (offset - offset / 2), high))\n",
        "\n",
        "    return low, high"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfC7ICEB1bBP"
      },
      "outputs": [],
      "source": [
        "def get_crop_window(face_location, height, width):\n",
        "    face_location = (face_location[0][3], height - face_location[0][0], face_location[0][1], height - face_location[0][2])\n",
        "\n",
        "    left, right = modified_normalize_face_borders(face_location[0], face_location[2], width)\n",
        "    bot, top = modified_normalize_face_borders(face_location[3], face_location[1], height)\n",
        "\n",
        "    face_location = (left, height - top, right, height - bot)\n",
        "\n",
        "    return face_location"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGAyR0Ap1bBP"
      },
      "source": [
        "## Testing Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg2Ld82R1bBP"
      },
      "outputs": [],
      "source": [
        "test_input = av.open('dataset_IFrames/Celeb-real/id0_0000.mp4')\n",
        "\n",
        "count = 0\n",
        "\n",
        "for frame in test_input.decode():\n",
        "    nd_frame = frame.to_ndarray()\n",
        "    img_frame = frame.to_image()\n",
        "\n",
        "    height, width = img_frame.height, img_frame.width\n",
        "\n",
        "    # Face location returned by face_recognition api: [(top, right, bottom, left)] in css terms\n",
        "    # Face location required by PIL.Image: (left, top, right, bottom)\n",
        "    face_location = face_recognition.api.face_locations(nd_frame)\n",
        "    face_location = get_crop_window(face_location, height, width)\n",
        "    \n",
        "    img_frame = img_frame.crop(face_location)\n",
        "    img_frame.save(f'dataset_face/Celeb-real/id0_0000_{count}.jpg')\n",
        "\n",
        "    count += 1 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IreSHjaJ1bBQ"
      },
      "source": [
        "### Turning Cropped faces to a Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Zx63O91bBQ",
        "outputId": "ac2878c7-6ccd-4ee9-d8ae-11b6598c4f22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:libav.mpeg4:bitrate tolerance 128000 too small for bitrate 1024000, overriding\n"
          ]
        }
      ],
      "source": [
        "test_input = av.open('dataset_IFrames/Celeb-real/id10_0001.mp4')\n",
        "test_output = av.open('dataset_face/Celeb-real/id10_0001.mp4', 'w')\n",
        "\n",
        "in_stream = test_input.streams.video[0]\n",
        "codec_name = in_stream.codec_context.name\n",
        "\n",
        "out_stream = test_output.add_stream(codec_name, 2)\n",
        "out_stream.width = in_stream.codec_context.width\n",
        "out_stream.height = in_stream.codec_context.height\n",
        "out_stream.pix_fmt = in_stream.codec_context.pix_fmt\n",
        "\n",
        "for frame in test_input.decode(in_stream):\n",
        "    img_frame = frame.to_image()\n",
        "    nd_frame = frame.to_ndarray()\n",
        "\n",
        "    height, width = img_frame.height, img_frame.width\n",
        "\n",
        "    # Face location returned by face_recognition api: [(top, right, bottom, left)]\n",
        "    # Face location required by PIL.Image: (left, top, right, bottom)\n",
        "    face_location = face_recognition.api.face_locations(nd_frame)\n",
        "    \n",
        "    if len(face_location) == 0:\n",
        "        continue\n",
        "\n",
        "    # face_location = (face_location[0][3], face_location[0][0], face_location[0][1], face_location[0][2])\n",
        "\n",
        "    # left, right = normalize_face_borders(face_location[0], face_location[2])\n",
        "    # bot, top = normalize_face_borders(face_location[3], face_location[1])\n",
        "    # face_location = (left, top, right, bot)\n",
        "\n",
        "    face_location = get_crop_window(face_location, height, width)\n",
        "    img_frame = img_frame.crop(face_location)\n",
        "\n",
        "    out_frame = av.VideoFrame.from_image(img_frame)\n",
        "    out_packet = out_stream.encode(out_frame)\n",
        "    test_output.mux(out_packet)\n",
        "\n",
        "out_packet = out_stream.encode(None)\n",
        "test_output.mux(out_packet)\n",
        "\n",
        "test_input.close()\n",
        "test_output.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fRkRM2M1bBQ"
      },
      "source": [
        "## Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZiLm08_1bBR"
      },
      "outputs": [],
      "source": [
        "def gpu_simple_save_cropped_faces_to_video(src_dir, dest_dir, vid_class, filename):\n",
        "    \n",
        "    input = av.open(src_dir + vid_class + filename)\n",
        "    output = av.open(dest_dir + vid_class + filename, 'w')\n",
        "\n",
        "    in_stream = input.streams.video[0]\n",
        "    codec_name = in_stream.codec_context.name\n",
        "\n",
        "    # output video dimension should be 256x256\n",
        "    out_stream = output.add_stream(codec_name, 2)\n",
        "    out_stream.width = 256\n",
        "    out_stream.height = 256\n",
        "    out_stream.pix_fmt = in_stream.codec_context.pix_fmt\n",
        "\n",
        "    frame_list = []\n",
        "    image_list = []\n",
        "    for frame in input.decode(in_stream):\n",
        "        frame_list.append(frame.to_ndarray())\n",
        "        image_list.append(frame.to_image())\n",
        "\n",
        "    face_locations = face_recognition.api.batch_face_locations(frame_list, 0)\n",
        "    for img_frame, face_location in zip(image_list, face_locations):\n",
        "        if len(face_location) == 0:\n",
        "            continue\n",
        "\n",
        "        face_location = (face_location[0][3], face_location[0][0], face_location[0][1], face_location[0][2])\n",
        "        img_frame = img_frame.crop(face_location)\n",
        "\n",
        "        out_frame = av.VideoFrame.from_image(img_frame)\n",
        "        out_packet = out_stream.encode(out_frame)\n",
        "        output.mux(out_packet)\n",
        "\n",
        "    out_packet = out_stream.encode(None)\n",
        "    output.mux(out_packet)\n",
        "\n",
        "    input.close()\n",
        "    output.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3cwEoRn1bBR"
      },
      "outputs": [],
      "source": [
        "for video in os.listdir(DS_IFRAME + CELEB_REAL):\n",
        "    gpu_simple_save_cropped_faces_to_video(DS_IFRAME, DS_FACE, CELEB_REAL, video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riRhCxB81bBS"
      },
      "outputs": [],
      "source": [
        "for video in os.listdir(DS_IFRAME + CELEB_FAKE):\n",
        "    gpu_simple_save_cropped_faces_to_video(DS_IFRAME, DS_FACE, CELEB_FAKE, video)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for video in os.listdir(DS_IFRAME + YT_REAL):\n",
        "    gpu_simple_save_cropped_faces_to_video(DS_IFRAME, DS_FACE, YT_REAL, video)"
      ],
      "metadata": {
        "id": "OO3N7qisB7gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MESONET**"
      ],
      "metadata": {
        "id": "IUOcxd9SQraT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "D-mUBvSLv6ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(input_size):\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv2D(input_shape=input_size, filters=8, kernel_size=3, activation='relu', padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2, padding=\"same\"))\n",
        "\n",
        "  model.add(layers.Conv2D(input_shape=(128, 128, 8), filters=8, kernel_size=5, activation='relu', padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2, padding=\"same\"))\n",
        "\n",
        "  \n",
        "  model.add(layers.Conv2D(input_shape=(64, 64, 8), filters=16, kernel_size=5, activation='relu', padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(4, 4, padding=\"same\"))\n",
        "\n",
        "  \n",
        "  model.add(layers.Conv2D(input_shape=(16, 16, 16), filters=16, kernel_size=5, activation='relu', padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(4, 4, padding=\"same\"))\n",
        "  \n",
        "  return model\n",
        "  "
      ],
      "metadata": {
        "id": "-XCfHamQQ0wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = (256, 256, 3)\n",
        "model = create_model(input_size)\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgfZce5lxvku",
        "outputId": "b0678b72-cbde-4405-f0af-e407e36fdc7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 256, 256, 8)       224       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256, 256, 8)      32        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 128, 128, 8)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 128, 128, 8)       1608      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 128, 128, 8)      32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 64, 64, 8)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 64, 16)        3216      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 64, 64, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 16, 16, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 16)        6416      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 4, 4, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11,656\n",
            "Trainable params: 11,560\n",
            "Non-trainable params: 96\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATASET CREATION**"
      ],
      "metadata": {
        "id": "IokdF4mk2w5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import imghdr\n",
        "from PIL import Image\n",
        "\n",
        "dir_real = 'dataset_REAL/'\n",
        "dir_fake = 'dataset_FAKE/'\n",
        "dir_train = './training_data/'\n",
        "dir_test = './testing_data/'\n",
        "\n",
        "if os.path.exists(dir_real) or os.path.exists(dir_fake):\n",
        "  shutil.rmtree(dir_real)\n",
        "  shutil.rmtree(dir_fake)\n",
        "os.makedirs(dir_real)\n",
        "os.makedirs(dir_fake)"
      ],
      "metadata": {
        "id": "ZKywoZ264zPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_extract_frames(source_path, dir_name, vid, count):\n",
        "  frame_count = 0;\n",
        "  jpg_encountered = bool(False)\n",
        "\n",
        "  if(imghdr.what(os.path.join(source_path, vid)) == 'jpeg'):\n",
        "    image = Image.open(source_path + vid)\n",
        "    image.save(f'./{dir_name}/vid_{count}_fr_{frame_count}.jpg')\n",
        "    jpg_encountered = bool(True)\n",
        "  \n",
        "  if(jpg_encountered):\n",
        "    return\n",
        "\n",
        "  vid = av.open(source_path + vid)\n",
        "\n",
        "  for frame in vid.decode():\n",
        "    image = frame.to_image()\n",
        "    image.save(f'./{dir_name}/vid_{count}_fr_{frame_count}.jpg')\n",
        "    frame_count += 1\n",
        "  frame_count = 0"
      ],
      "metadata": {
        "id": "QBi0phmw3ZbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting frames from Celeb-real face-cropped data\n",
        "vid_count = 1;\n",
        "source_path = DS_FACE + CELEB_REAL\n",
        "for video in os.listdir(source_path):\n",
        "  print(video, vid_count)\n",
        "  dataset_extract_frames(source_path, dir_real, video, vid_count)\n",
        "  vid_count += 1"
      ],
      "metadata": {
        "id": "93EnsQWhfSsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting frames from YouTube-real face-cropped data\n",
        "source_path = DS_FACE + YT_REAL\n",
        "for video in os.listdir(source_path):\n",
        "  print(video, vid_count)\n",
        "  dataset_extract_frames(source_path, dir_real, video, vid_count)\n",
        "  vid_count += 1"
      ],
      "metadata": {
        "id": "ZALiQjgu84s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting frames from Celeb-synthesis face-cropped data\n",
        "vid_count = 1\n",
        "source_path = DS_FACE + CELEB_FAKE\n",
        "for video in os.listdir(source_path):\n",
        "  print(video, vid_count)\n",
        "  dataset_extract_frames(source_path, dir_fake, video, vid_count)\n",
        "  vid_count += 1"
      ],
      "metadata": {
        "id": "WRw_64n1PnI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting test data\n",
        "\n",
        "def extract_test_data(source_path):\n",
        "  frame_list = []\n",
        "  for frame in os.listdir(source_path):\n",
        "    frame_path = os.path.join(source_path, frame)\n",
        "    frame_list.append(frame_path)\n",
        "\n",
        "  size = int(20/100 * len(frame_list))\n",
        "  sampled_list = random.sample(frame_list, size)\n",
        "  return sampled_list"
      ],
      "metadata": {
        "id": "2Uks_VAlvLb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting test data from real dataset\n",
        "\n",
        "source_path = dir_train + dir_real\n",
        "sampled_list = extract_test_data(source_path)\n",
        "for frame_path in sampled_list:\n",
        "  shutil.copy(frame_path, os.path.join(dir_test, os.path.basename(frame_path)))\n",
        "  os.remove(frame_path)"
      ],
      "metadata": {
        "id": "qYikegxkYQ5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting test data from fake dataset\n",
        "\n",
        "source_path = dir_train + dir_fake\n",
        "sampled_list = extract_test_data(source_path)\n",
        "for frame_path in sampled_list:\n",
        "  shutil.copy(frame_path, os.path.join(dir_test, os.path.basename(frame_path)))\n",
        "  os.remove(frame_path)"
      ],
      "metadata": {
        "id": "Xs2o57qR0blt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dataset from folders\n",
        "def create_dataset(dir_path):\n",
        "  ds = keras.utils.image_dataset_from_directory(\n",
        "      directory = dir_path,\n",
        "      labels = 'inferred',\n",
        "      label_mode = 'binary',\n",
        "      batch_size = 32,\n",
        "      color_mode = 'rgb',\n",
        "      shuffle = True,\n",
        "      validation_split = 0.2,\n",
        "      subset = 'validation',\n",
        "      seed = 1\n",
        "  )\n",
        "  return ds"
      ],
      "metadata": {
        "id": "NseO3UFkCBWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = create_dataset(dir_train)\n",
        "for data, labels in train_ds.take(1):\n",
        "  print(data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "2xQOGd1_FBfE",
        "outputId": "28d5f1d8-238a-4829-9ac7-2f35a331cea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 29535 files belonging to 2 classes.\n",
            "Using 5907 files for validation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ad201d470736>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    785\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;31m# to communicate that there is no more data to iterate over.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSYNC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m       ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[1;32m    771\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m           \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3010\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3011\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3012\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3013\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"IteratorGetNext\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3014\u001b[0m         \"output_shapes\", output_shapes)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "634b966a65970263d64a39f28ca9ed21ee1352c353852874a3701cf42e66fa53"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}