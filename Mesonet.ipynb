{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq9_NfM01bAv"
      },
      "source": [
        "# Libraries Used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QE15KxFc1bA7"
      },
      "source": [
        "* ffmpeg-python\n",
        "* av\n",
        "* cmake\n",
        "* dlib  (based on the python version)\n",
        "* face-recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCNM7aYyTUOe",
        "outputId": "fd408927-101f-47c9-92b5-51102da6b1bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoaPfQnyRwnf",
        "outputId": "4d8a19fb-7150-49a3-dd1c-ed5a5312bfdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/FYP/Celeb DF v1 Dataset\n"
          ]
        }
      ],
      "source": [
        "%cd drive/MyDrive/FYP/Celeb DF\\ v1\\ Dataset/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "II83t9mmHtWd"
      },
      "outputs": [],
      "source": [
        "!unzip dataset_mesonet.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QY1acu8nB5-9"
      },
      "outputs": [],
      "source": [
        "%pip install ffmpeg-python\n",
        "%pip install av\n",
        "%pip install cmake\n",
        "%pip install dlib\n",
        "%pip install face-recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUOcxd9SQraT"
      },
      "source": [
        "# **MESONET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D-mUBvSLv6ZN"
      },
      "outputs": [],
      "source": [
        "import av\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import shutil\n",
        "import imghdr\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iA_RZXqHTUOj"
      },
      "outputs": [],
      "source": [
        "DS_CDFV1 = 'celeb_df_v1/'\n",
        "DS_CDFV2 = 'celeb_df_v2/'\n",
        "\n",
        "DS_ORGINAL = 'dataset_original/'\n",
        "DS_SPLIT = 'dataset_split/'\n",
        "DS_IFRAMES = 'dataset_iframes/'\n",
        "DS_FACE = 'dataset_face/'\n",
        "DS_FACE_IMG = 'dataset_face_img/'\n",
        "DS_SRM_SNIPPETS = 'dataset_srm_snippets_5/'\n",
        "DS_SEGMENTS = 'dataset_segments/'\n",
        "DS_RAW = 'dataset_raw/'\n",
        "DS_RESIDUALS = 'dataset_residuals/'\n",
        "DS_TEMPORAL = 'dataset_temporal/'\n",
        "\n",
        "\n",
        "SEG_1 = 'seg_1/'\n",
        "SEG_2 = 'seg_2/'\n",
        "SEG_3 = 'seg_3/'\n",
        "SEG_4 = 'seg_4/'\n",
        "SEG_5 = 'seg_5/'\n",
        "\n",
        "SEG = ['seg_1_', 'seg_2_', 'seg_3_', 'seg_4_', 'seg_5_']\n",
        "\n",
        "DS_TRAIN = 'train_dataset/'\n",
        "DS_TEST = 'test_dataset/'\n",
        "DS_VAL = 'val_dataset/'\n",
        "\n",
        "CLASS_FAKE = 'fake/'\n",
        "CLASS_REAL = 'real/'\n",
        "\n",
        "\n",
        "TOP_LEVEL_1 = [DS_SPLIT, DS_IFRAMES, DS_FACE, DS_FACE_IMG, DS_SRM_SNIPPETS]\n",
        "TOP_LEVEL_2 = [DS_SEGMENTS, DS_RAW, DS_RESIDUALS]\n",
        "SEGMENTS = [SEG_1, SEG_2, SEG_3, SEG_4, SEG_5]\n",
        "SPLIT = [DS_TRAIN, DS_TEST, DS_VAL]\n",
        "CLASS = [CLASS_REAL, CLASS_FAKE]\n",
        "\n",
        "DATASET = [DS_CDFV1, DS_CDFV2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-XCfHamQQ0wc"
      },
      "outputs": [],
      "source": [
        "def create_model(input_size):\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  model.add(layers.Conv2D(input_shape=input_size, filters=8, kernel_size=3, activation='relu', padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2, padding=\"same\"))\n",
        "\n",
        "  model.add(layers.Conv2D(input_shape=(128, 128, 8), filters=8, kernel_size=5, activation='relu', padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(2, 2, padding=\"same\"))\n",
        "\n",
        "  \n",
        "  model.add(layers.Conv2D(input_shape=(64, 64, 8), filters=16, kernel_size=5, activation='relu', padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(4, 4, padding=\"same\"))\n",
        "\n",
        "  \n",
        "  model.add(layers.Conv2D(input_shape=(16, 16, 16), filters=16, kernel_size=5, activation='relu', padding=\"same\"))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(MaxPool2D(4, 4, padding=\"same\"))\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(layers.Dense(16))\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(layers.Dense(1, activation='sigmoid'))\n",
        "  \n",
        "  return model\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgfZce5lxvku",
        "outputId": "13817b37-147b-4478-f022-1a57c361efbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 256, 256, 8)       224       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 256, 256, 8)      32        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 128, 128, 8)      0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 128, 128, 8)       1608      \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 128, 128, 8)      32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 64, 64, 8)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 64, 64, 16)        3216      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 64, 64, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 16, 16, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 16)        6416      \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 16, 16, 16)       64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 4, 4, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 256)               0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                4112      \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 16)                0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,785\n",
            "Trainable params: 15,689\n",
            "Non-trainable params: 96\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "input_size = (256, 256, 3)\n",
        "model = create_model(input_size)\n",
        "model.compile(optimizer=Adam(learning_rate=0.0001), \n",
        "              loss='binary_crossentropy', \n",
        "              metrics = [keras.metrics.BinaryAccuracy(), \n",
        "                         keras.metrics.Precision(), \n",
        "                         keras.metrics.Recall(),\n",
        "                         keras.metrics.AUC()])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IokdF4mk2w5a"
      },
      "source": [
        "## Frame Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBi0phmw3ZbK"
      },
      "outputs": [],
      "source": [
        "def extract_frames_from_videos(src_path, dst_path, video, count):\n",
        "  frame_count = 0\n",
        "\n",
        "  # \n",
        "\n",
        "  pass\n",
        "\n",
        "def dataset_extract_frames(source_path, dest_path, vid, tag, count):\n",
        "  frame_count = 0\n",
        "\n",
        "  if(imghdr.what(os.path.join(source_path, vid)) == 'jpeg'):\n",
        "    image = Image.open(source_path + vid)\n",
        "    image.save(f'{dest_path}/vid_{tag}{count}_fr_{frame_count}.jpg')\n",
        "    \n",
        "    return\n",
        "\n",
        "  vid = av.open(source_path + vid)\n",
        "  for frame in vid.decode():\n",
        "    image = frame.to_image()\n",
        "    image.save(f'{dest_path}/vid_{tag}{count}_fr_{frame_count}.jpg')\n",
        "\n",
        "    frame_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93EnsQWhfSsm"
      },
      "outputs": [],
      "source": [
        "#extracting frames from Celeb-real face-cropped data\n",
        "vid_count = 1\n",
        "source_path = DS_FACE + CELEB_REAL\n",
        "for video in os.listdir(source_path):\n",
        "  # print(video, vid_count)\n",
        "  dataset_extract_frames(source_path, MS_TRAIN + REAL_VIDS, video, 'cr', vid_count)\n",
        "  vid_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZALiQjgu84s2"
      },
      "outputs": [],
      "source": [
        "#extracting frames from YouTube-real face-cropped data\n",
        "vid_count = 1\n",
        "source_path = DS_FACE + YT_REAL\n",
        "for video in os.listdir(source_path):\n",
        "  # print(video, vid_count)\n",
        "  dataset_extract_frames(source_path, MS_TRAIN + REAL_VIDS, video, 'yr', vid_count)\n",
        "  vid_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRw_64n1PnI4"
      },
      "outputs": [],
      "source": [
        "#extracting frames from Celeb-synthesis face-cropped data\n",
        "vid_count = 1\n",
        "source_path = DS_FACE + CELEB_FAKE\n",
        "for video in os.listdir(source_path):\n",
        "  # print(video, vid_count)\n",
        "  dataset_extract_frames(source_path, MS_TRAIN + FAKE_VIDS, video, 'cf', vid_count)\n",
        "  vid_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Uks_VAlvLb2"
      },
      "outputs": [],
      "source": [
        "#extracting test data\n",
        "\n",
        "def extract_test_data(source_path):\n",
        "  frame_list = []\n",
        "  for frame in os.listdir(source_path):\n",
        "    frame_path = os.path.join(source_path, frame)\n",
        "    frame_list.append(frame_path)\n",
        "\n",
        "  size = int(20/100 * len(frame_list))\n",
        "  sampled_list = random.sample(frame_list, size)\n",
        "  return sampled_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYikegxkYQ5x"
      },
      "outputs": [],
      "source": [
        "#extracting test data from real dataset\n",
        "\n",
        "source_path = MS_TRAIN + REAL_VIDS\n",
        "sampled_list = extract_test_data(source_path)\n",
        "for frame_path in sampled_list:\n",
        "  shutil.copy(frame_path, os.path.join(MS_TEST + REAL_VIDS, os.path.basename(frame_path)))\n",
        "  os.remove(frame_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xs2o57qR0blt"
      },
      "outputs": [],
      "source": [
        "#extracting test data from fake dataset\n",
        "\n",
        "source_path = MS_TRAIN + FAKE_VIDS\n",
        "sampled_list = extract_test_data(source_path)\n",
        "for frame_path in sampled_list:\n",
        "  shutil.copy(frame_path, os.path.join(MS_TEST + FAKE_VIDS, os.path.basename(frame_path)))\n",
        "  os.remove(frame_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtgPgNgAFA47"
      },
      "source": [
        "## Dataset Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NseO3UFkCBWz"
      },
      "outputs": [],
      "source": [
        "#creating dataset from folders\n",
        "def create_dataset(dir_path):\n",
        "  ds = keras.utils.image_dataset_from_directory(\n",
        "      directory = dir_path,\n",
        "      labels = 'inferred',\n",
        "      label_mode = 'binary',\n",
        "      batch_size = 32,\n",
        "      color_mode = 'rgb',\n",
        "      shuffle = True,\n",
        "      validation_split = 0.2,\n",
        "      subset = 'validation',\n",
        "      seed = 1\n",
        "  )\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xQOGd1_FBfE",
        "outputId": "d9cd08e7-9b2f-481d-f290-693eb2e701e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 22726 files belonging to 2 classes.\n",
            "Using 18181 files for training.\n",
            "Using 4545 files for validation.\n",
            "(32, 256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "(train_ds, val_ds) = keras.utils.image_dataset_from_directory(\n",
        "      directory = MS_TRAIN,\n",
        "      labels = 'inferred',\n",
        "      label_mode = 'binary',\n",
        "      batch_size = 32,\n",
        "      color_mode = 'rgb',\n",
        "      shuffle = True,\n",
        "      validation_split = 0.2,\n",
        "      subset = 'both',\n",
        "      seed = 1\n",
        "  )\n",
        "for data, labels in train_ds.take(1):\n",
        "  print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U971iBD4TUOq",
        "outputId": "2be33ae4-5c13-4e79-9a16-803e33af660a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5680 files belonging to 2 classes.\n",
            "(32, 256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "test_ds = keras.utils.image_dataset_from_directory(\n",
        "    directory=MS_TEST,\n",
        "    labels='inferred',\n",
        "    label_mode='binary'\n",
        ")\n",
        "\n",
        "for data, labels in test_ds.take(1):\n",
        "  print(data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-O4Wwk4GFA48"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUK2ROFsFA49"
      },
      "outputs": [],
      "source": [
        "# Load from checkpoint (if exists)\n",
        "try:\n",
        "    saved_model = keras.models.load_model(MS_MODEL)\n",
        "    model = saved_model\n",
        "\n",
        "except IOError:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKKOsMgFWYfM"
      },
      "outputs": [],
      "source": [
        "max_epochs = 20\n",
        "\n",
        "model.fit(train_ds, \n",
        "          epochs=max_epochs, \n",
        "          validation_data=val_ds,\n",
        "          callbacks=keras.callbacks.ModelCheckpoint(MS_MODEL),\n",
        "          verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEZVf-XUFA49"
      },
      "source": [
        "## Test dataset metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjaGCzKgFA4-",
        "outputId": "bce59c19-88f8-41df-99b9-5965325e3058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "178/178 [==============================] - 13s 68ms/step - loss: 0.2865 - binary_accuracy: 0.8798 - precision: 0.9113 - recall: 0.8502 - auc: 0.9538\n",
            "Loss:\t0.2865\n",
            "Accuracy:\t0.8798\n",
            "Precision:\t0.9113\n",
            "Recall:\t0.8502\n",
            "AUC:\t0.8502\n"
          ]
        }
      ],
      "source": [
        "loss, acc, prec, rec, auc = model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss:\\t{loss:.4f}')\n",
        "print(f'Accuracy:\\t{acc:.4f}')\n",
        "print(f'Precision:\\t{prec:.4f}')\n",
        "print(f'Recall:\\t{rec:.4f}')\n",
        "print(f'AUC:\\t{rec:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXAgr_Edl_hY"
      },
      "source": [
        "## Testing Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l8Mxg3VX8Jz",
        "outputId": "641c1bf0-5768-440c-dc7f-c48b4bcb03a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f88a1598520>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_model = keras.models.load_model(MS_MODEL)\n",
        "new_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tu_gsZ3mlvND"
      },
      "outputs": [],
      "source": [
        "new_model.fit(train_ds, \n",
        "          epochs=5, \n",
        "          validation_data=val_ds,\n",
        "          callbacks=keras.callbacks.ModelCheckpoint(MS_MODEL),\n",
        "          verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIYZysVCmK6L",
        "outputId": "14051316-89e2-48e4-9aae-ac49697b69cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "178/178 [==============================] - 11s 61ms/step - loss: 0.2865 - binary_accuracy: 0.8798 - precision: 0.9113 - recall: 0.8502 - auc: 0.9538\n",
            "Loss: 0.2865\n",
            "Accuracy: 0.8798\n",
            "Precision: 0.9113\n",
            "Recall: 0.8502\n",
            "AUC: 0.9538\n"
          ]
        }
      ],
      "source": [
        "loss, acc, prec, rec, auc = new_model.evaluate(test_ds)\n",
        "\n",
        "print(f'Loss: {loss:.4f}')\n",
        "print(f'Accuracy: {acc:.4f}')\n",
        "print(f'Precision: {prec:.4f}')\n",
        "print(f'Recall: {rec:.4f}')\n",
        "print(f'AUC: {auc:.4f}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "634b966a65970263d64a39f28ca9ed21ee1352c353852874a3701cf42e66fa53"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
