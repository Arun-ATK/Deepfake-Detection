{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization, Dropout, Layer\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_CDFV1 = 'celeb_df_v1/'\n",
    "DS_CDFV2 = 'celeb_df_v2/'\n",
    "\n",
    "DS_ORGINAL = 'dataset_original/'\n",
    "DS_SPLIT = 'dataset_split/'\n",
    "DS_IFRAMES = 'dataset_iframes/'\n",
    "DS_FACE = 'dataset_face/'\n",
    "DS_FACE_IMG = 'dataset_face_img/'\n",
    "DS_SRM_SNIPPETS = 'dataset_srm_snippets_5/'\n",
    "DS_SEGMENTS = 'dataset_segments/'\n",
    "DS_RAW = 'dataset_raw/'\n",
    "DS_RESIDUALS = 'dataset_residuals/'\n",
    "\n",
    "SEG_1 = 'seg_1/'\n",
    "SEG_2 = 'seg_2/'\n",
    "SEG_3 = 'seg_3/'\n",
    "SEG_4 = 'seg_4/'\n",
    "SEG_5 = 'seg_5/'\n",
    "\n",
    "DS_TRAIN = 'train_dataset/'\n",
    "DS_TEST = 'test_dataset/'\n",
    "DS_VAL = 'val_dataset/'\n",
    "\n",
    "CLASS_FAKE = 'fake/'\n",
    "CLASS_REAL = 'real/'\n",
    "\n",
    "\n",
    "TOP_LEVEL_1 = [DS_SPLIT, DS_IFRAMES, DS_FACE, DS_FACE_IMG, DS_SRM_SNIPPETS]\n",
    "TOP_LEVEL_2 = [DS_SEGMENTS, DS_RAW, DS_RESIDUALS]\n",
    "SEGMENTS = [SEG_1, SEG_2, SEG_3, SEG_4, SEG_5]\n",
    "SPLIT = [DS_TRAIN, DS_TEST, DS_VAL]\n",
    "CLASS = [CLASS_REAL, CLASS_FAKE]\n",
    "\n",
    "DATASET = [DS_CDFV1, DS_CDFV2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snippet Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the index of frames that begin a new segment (except the first segment)\n",
    "def get_segment_dividers(frame_count, num_segments):\n",
    "    segments_per_frame = math.floor(frame_count / num_segments)\n",
    "\n",
    "    return [(segments_per_frame * i) for i in range(1, num_segments) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the indices of the frames that will be randomly selected from each segment\n",
    "# Multiple snippets indices per segment can be returned by setting the num_snippets arg \n",
    "def get_snippet_indices(segment_dividers, num_snippets):\n",
    "    start_index = 0\n",
    "    num_snippets = 1 if num_snippets <= 0 else num_snippets\n",
    "\n",
    "    snippet_indices = []\n",
    "    for end_index in segment_dividers:\n",
    "\n",
    "        # Extracting multiple snippets per segment (if needed)\n",
    "        for _ in range(num_snippets):\n",
    "            snippet_indices.append(random.randint(start_index, end_index - 1))\n",
    "\n",
    "        start_index = end_index\n",
    "        \n",
    "    return snippet_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array of randomly selected snippets(PIL.Image) from each segment of the input video\n",
    "def extract_snippets(fp, num_segments, num_snippets):\n",
    "    vid_container = av.open(fp)\n",
    "    vid_stream = vid_container.streams.video[0]\n",
    "    frame_count = vid_stream.frames\n",
    "\n",
    "    snippets = []\n",
    "\n",
    "    # If number of frames in video is less than the number of frames that need to sampled\n",
    "    # then take all frames in the video\n",
    "    if frame_count < num_segments * num_snippets:\n",
    "        for frame in vid_container.decode():\n",
    "            snippets.append(frame.to_image())\n",
    "\n",
    "    else:\n",
    "        segment_dividers = get_segment_dividers(frame_count, num_segments)\n",
    "        segment_dividers = segment_dividers + [frame_count]\n",
    "\n",
    "        snippet_indices = get_snippet_indices(segment_dividers, num_snippets)\n",
    "\n",
    "        frame_index = 0\n",
    "        for frame in vid_container.decode():\n",
    "            if frame_index > max(snippet_indices):\n",
    "                break\n",
    "\n",
    "            if frame_index in snippet_indices:\n",
    "                snippets.append(frame.to_image())\n",
    "\n",
    "            frame_index += 1\n",
    "\n",
    "    return snippets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segment Dividers: [10, 20]\n",
      "Snippets[4, 9, 15, 13, 20, 26]\n"
     ]
    }
   ],
   "source": [
    "tmp_count = 30\n",
    "tmp_seg = get_segment_dividers(tmp_count, 3)\n",
    "tmp_snip = get_snippet_indices(tmp_seg + [tmp_count], 2)\n",
    "\n",
    "print(f'Segment Dividers: {tmp_seg}')\n",
    "print(f'Snippets{tmp_snip}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# test_file = os.listdir(DS_CDFV1 + DS_SPLIT + DS_TRAIN + CLASS_REAL)[0]\n",
    "test_file = DS_CDFV2 + DS_SPLIT + DS_TRAIN + CLASS_REAL + 'id27_0005.mp4'\n",
    "# test_input = av.open(os.path.realpath(DS_CDFV1 + DS_SPLIT + DS_TRAIN + CLASS_REAL + test_file))\n",
    "test_input = av.open(test_file)\n",
    "\n",
    "print(test_input.streams.video[0].frames)\n",
    "\n",
    "# for frame in test_input.decode():\n",
    "#     print(frame.key_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = os.listdir(DS_CDFV1 + DS_FACE + DS_TRAIN + CLASS_REAL)[0]\n",
    "test_file = DS_CDFV2 + DS_FACE + DS_TRAIN + CLASS_REAL + 'id27_0005.mp4'\n",
    "# test_file = DS_CDFV1 + DS_FACE + DS_TRAIN + CLASS_REAL + test_file\n",
    "tmp_snippets = extract_snippets(test_file, 5, 1)\n",
    "\n",
    "for s in tmp_snippets:\n",
    "    s.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Celeb-DF v1 & v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_snippets_CDF(dataset, num_segments, num_snippets):\n",
    "    if dataset != DS_CDFV1 and dataset != DS_CDFV2:\n",
    "        print(dataset)\n",
    "        return\n",
    "    \n",
    "    random.seed(1)\n",
    "    \n",
    "    src_base_path = dataset + DS_FACE\n",
    "    dst_base_path = dataset + DS_SRM_SNIPPETS\n",
    "\n",
    "    for split in SPLIT:\n",
    "        print(f'---Split started: {split}---')\n",
    "        for class_dir in CLASS:\n",
    "            print(f'Class started: {class_dir}')\n",
    "\n",
    "            for video in os.listdir(src_base_path + split + class_dir):\n",
    "                fp = src_base_path + split + class_dir + video\n",
    "                snippets = extract_snippets(fp, num_segments, num_snippets)\n",
    "\n",
    "                for i, snippet in enumerate(snippets, start=1):\n",
    "                    seg_index = math.ceil(float(i) / num_snippets)\n",
    "                    snip_index = (i - 1) % num_snippets\n",
    "              \n",
    "                    dst = f'{dst_base_path + split + class_dir + os.path.splitext(video)[0]}_s{seg_index}_f{snip_index}.jpeg'\n",
    "                    snippet.save(dst)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Split started: train_dataset/---\n",
      "Class started: real/\n",
      "Class started: fake/\n",
      "---Split started: test_dataset/---\n",
      "Class started: real/\n",
      "Class started: fake/\n",
      "---Split started: val_dataset/---\n",
      "Class started: real/\n",
      "Class started: fake/\n"
     ]
    }
   ],
   "source": [
    "# CELEB DF V1\n",
    "save_snippets_CDF(DS_CDFV1, num_segments=5, num_snippets=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Split started: train_dataset/---\n",
      "Class started: real/\n",
      "Class started: fake/\n",
      "---Split started: test_dataset/---\n",
      "Class started: real/\n",
      "Class started: fake/\n",
      "---Split started: val_dataset/---\n",
      "Class started: real/\n",
      "Class started: fake/\n"
     ]
    }
   ],
   "source": [
    "# CELEB DF V2\n",
    "save_snippets_CDF(DS_CDFV2, num_segments=5, num_snippets=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor_dataset(dataset, split):\n",
    "    ds = keras.utils.image_dataset_from_directory(\n",
    "        directory = dataset + DS_SRM_SNIPPETS + split,\n",
    "        labels = 'inferred',\n",
    "        label_mode = 'binary',\n",
    "        batch_size = 32,\n",
    "        color_mode = 'rgb',\n",
    "        shuffle = True,\n",
    "        seed = 1\n",
    "    )\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celeb DF v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4415 files belonging to 2 classes.\n",
      "Found 500 files belonging to 2 classes.\n",
      "Found 1100 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset_cdfv1 = create_tensor_dataset(DS_CDFV1, DS_TRAIN)\n",
    "test_dataset_cdfv1 = create_tensor_dataset(DS_CDFV1, DS_TEST)\n",
    "val_dataset_cdfv1 = create_tensor_dataset(DS_CDFV1, DS_VAL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Celeb DF v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24046 files belonging to 2 classes.\n",
      "Found 2590 files belonging to 2 classes.\n",
      "Found 6005 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset_cdfv2 = create_tensor_dataset(DS_CDFV2, DS_TRAIN)\n",
    "test_dataset_cdfv2 = create_tensor_dataset(DS_CDFV2, DS_TEST)\n",
    "val_dataset_cdfv2 = create_tensor_dataset(DS_CDFV2, DS_VAL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRMLayer(Layer):\n",
    "    def __init__(self, filters, kernel_size, fixed_filters, **kwargs):\n",
    "        super(SRMLayer, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.fixed_filters = fixed_filters\n",
    "\n",
    "        self.q_small = self.add_weight(name='q_small',\n",
    "                                       shape=(),\n",
    "                                       initializer=keras.initializers.Constant(value=2.0),\n",
    "                                       trainable=True)\n",
    "        \n",
    "        self.q_med = self.add_weight(name='q_med',\n",
    "                                     shape=(),\n",
    "                                     initializer=keras.initializers.Constant(value=4.0),\n",
    "                                     trainable=True)\n",
    "        \n",
    "        self.q_large = self.add_weight(name='q_large',\n",
    "                                       shape=(),\n",
    "                                       initializer=keras.initializers.Constant(value=12.0),\n",
    "                                       trainable=True)\n",
    "        \n",
    "        def build(self, input_shape):\n",
    "            super(SRMLayer, self).build(input_shape)\n",
    "            fixed_filters = tf.constant(self.fixed_filters, dtype=tf.float32)\n",
    "            print(fixed_filters)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            fixed_filters = tf.constant(self.fixed_filters, dtype=tf.float32)\n",
    "\n",
    "            # print(fixed_filters[:, :, 0:1, :])\n",
    "\n",
    "            output1 = tf.nn.conv2d(inputs, fixed_filters[:, :, 0:1, :], strides=[1, 1, 1, 1], padding='SAME')\n",
    "            output2 = tf.nn.conv2d(inputs, fixed_filters[:, :, 1:2, :], strides=[1, 1, 1, 1], padding='SAME')\n",
    "            output3 = tf.nn.conv2d(inputs, fixed_filters[:, :, 2:3, :], strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "            output1 /= self.q_small\n",
    "            output2 /= self.q_med\n",
    "            output3 /= self.q_large\n",
    "\n",
    "            output = tf.concat([output1, output2, output3], axis=3)\n",
    "            return output\n",
    "        \n",
    "        def compute_output_shape(self, input_shape):\n",
    "            batch_size = input_shape[0]\n",
    "            height = input_shape[1]\n",
    "            width = input_shape[2]\n",
    "\n",
    "        def get_config(self):\n",
    "            config = super(SRMLayer, self).get_config()\n",
    "            config.update({'filters': self.filters,\n",
    "                           'kernel_size': self.kernel_size,\n",
    "                           'fixed_filters': self.fixed_filters})\n",
    "            \n",
    "            return config\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Logic"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For Convolution**\n",
    "- Input Shape:  [batch_size, height, width, channels]\n",
    "- Filter Shape: [height, width, in_channels, filter_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorShape([5, 5, 3, 2])\n",
      "(32, 252, 252, 2)\n"
     ]
    }
   ],
   "source": [
    "tmp_filter_small = tf.constant([[0, 0,  0, 0, 0],\n",
    "                                [0, 0,  0, 0, 0],\n",
    "                                [0, 1, -2, 1, 0],\n",
    "                                [0, 0,  0, 0, 0],\n",
    "                                [0, 0,  0, 0, 0]], dtype=tf.float32)\n",
    "\n",
    "tmp_filter_med = tf.constant([[0,  0,  0,  0, 0],\n",
    "                              [0, -1,  2, -1, 0],\n",
    "                              [0,  2, -4,  2, 0],\n",
    "                              [0, -1,  2, -1, 0],\n",
    "                              [0,  0,  0,  0, 0]], dtype=tf.float32)\n",
    "\n",
    "tmp_filter_small = tf.expand_dims(tf.expand_dims(tmp_filter_small, axis=-1), axis=-1)\n",
    "tmp_filter_med   = tf.expand_dims(tf.expand_dims(tmp_filter_med,   axis=-1), axis=-1)\n",
    "# tf.print(tmp_filter_small.shape)\n",
    "\n",
    "tmp_filter_small = tf.tile(tmp_filter_small, [1, 1, 3, 1])\n",
    "tmp_filter_med   = tf.tile(tmp_filter_med,   [1, 1, 3, 1])\n",
    "\n",
    "tmp_filters = tf.concat([tmp_filter_small, tmp_filter_med], axis=3)\n",
    "\n",
    "tf.print(tmp_filters.shape)\n",
    "\n",
    "tmp_conv = Conv2D(filters=2, \n",
    "                  kernel_size=5, \n",
    "                  kernel_initializer=tf.keras.initializers.Constant(tmp_filters))\n",
    "\n",
    "tmp_x = tf.random.normal([32, 256, 256, 3])\n",
    "tmp_y = tmp_conv(tmp_x)\n",
    "\n",
    "print(tmp_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1]\n",
      " [0.1 0.1 0.1 0.1 0.1]], shape=(5, 5), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.  10. -20.  10.   0.]\n",
      " [  0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.]], shape=(5, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tmp_filter_small = tf.constant([[0, 0,  0, 0, 0],\n",
    "                                [0, 0,  0, 0, 0],\n",
    "                                [0, 1, -2, 1, 0],\n",
    "                                [0, 0,  0, 0, 0],\n",
    "                                [0, 0,  0, 0, 0]], dtype=tf.float32)\n",
    "\n",
    "tmp_q = tf.constant([0], dtype=tf.float32)\n",
    "tmp_nz = tf.constant([0.1], dtype=tf.float32)\n",
    "\n",
    "tmp_q  = tf.expand_dims(tmp_q,  axis=-1)\n",
    "tmp_nz = tf.expand_dims(tmp_nz, axis=-1)\n",
    "\n",
    "tmp_q  = tf.tile(tmp_q, [5, 5])\n",
    "tmp_nz = tf.tile(tmp_nz, [5, 5])\n",
    "\n",
    "print(tmp_q)\n",
    "tmp_q = tf.where(tf.equal(tmp_q, 0), tmp_nz, tmp_q)\n",
    "print(tmp_q)\n",
    "\n",
    "\n",
    "tmp_filter_small = tf.math.divide(tmp_filter_small, tmp_q)\n",
    "\n",
    "print(tmp_filter_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  1 -2  1  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]], shape=(5, 5), dtype=int32)\n",
      "tf.Tensor([2], shape=(1,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.5 -1.   0.5  0. ]\n",
      " [ 0.   0.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0.   0. ]], shape=(5, 5), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "tmp_x = tf.constant([[0, 0,  0, 0, 0],\n",
    "                                [0, 0,  0, 0, 0],\n",
    "                                [0, 1, -2, 1, 0],\n",
    "                                [0, 0,  0, 0, 0],\n",
    "                                [0, 0,  0, 0, 0]])\n",
    "tmp_y = tf.constant([2])\n",
    "\n",
    "tmp_res = tf.divide(tmp_x, tmp_y)\n",
    "\n",
    "print(tmp_x)\n",
    "print(tmp_y)\n",
    "print(tmp_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom SRM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSRM(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SSRM, self).__init__(**kwargs)\n",
    "\n",
    "        self.filter_small = tf.constant([[0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 1, -2, 1, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0]], dtype=tf.float32)\n",
    "        \n",
    "        self.filter_med = tf.constant([[0,  0,  0,  0, 0],\n",
    "                                       [0, -1,  2, -1, 0],\n",
    "                                       [0,  2, -4,  2, 0],\n",
    "                                       [0, -1,  2, -1, 0],\n",
    "                                       [0,  0,  0,  0, 0]], dtype=tf.float32)\n",
    "        \n",
    "        self.filter_large = tf.constant([[-1,  2,  -2,  2, -1],\n",
    "                                         [ 2, -6,   8, -6,  2],\n",
    "                                         [-2,  8, -12,  8, -2],\n",
    "                                         [ 2, -6,   8, -6,  2],\n",
    "                                         [-1,  2,  -2,  2, -1]], dtype=tf.float32)\n",
    "        \n",
    "        self.filter_small = tf.expand_dims(tf.expand_dims(self.filter_small, axis=-1), axis=-1)\n",
    "        self.filter_med   = tf.expand_dims(tf.expand_dims(self.filter_med,   axis=-1), axis=-1)\n",
    "        self.filter_large = tf.expand_dims(tf.expand_dims(self.filter_large, axis=-1), axis=-1)\n",
    "\n",
    "        self.filter_small = tf.tile(self.filter_small, [1, 1, 3, 1])\n",
    "        self.filter_med   = tf.tile(self.filter_med,   [1, 1, 3, 1])\n",
    "        self.filter_large = tf.tile(self.filter_large, [1, 1, 3, 1])\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            output_small = tf.nn.conv2d(inputs, self.filter_small, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            output_med   = tf.nn.conv2d(inputs, self.filter_med,   strides=[1, 1, 1, 1], padding='SAME')\n",
    "            output_large = tf.nn.conv2d(inputs, self.filter_large, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "            return tf.concat([output_small, output_med, output_large], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSRM(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PSRM, self).__init__(**kwargs)\n",
    "\n",
    "        self.filter_small = tf.constant([[0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 1, -2, 1, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0]], dtype=tf.float32)\n",
    "        \n",
    "        self.filter_small = tf.expand_dims(tf.expand_dims(self.filter_small, axis=-1), axis=-1)\n",
    "        self.filter_small = tf.tile(self.filter_small, [1, 1, 3, 1])\n",
    "\n",
    "        self.q_small = self.add_weight(name = 'q_small',\n",
    "                                       shape = (1, ), \n",
    "                                       initializer = tf.initializers.Constant(value=2.0),\n",
    "                                       trainable = True)\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            filter_small = tf.math.divide(self.filter_small, self.q_small)\n",
    "            output_small = tf.nn.conv2d(inputs, filter_small, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "            return output_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSRM_3(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PSRM_3, self).__init__(**kwargs)\n",
    "\n",
    "        self.filter_small = tf.constant([[0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 1, -2, 1, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0]], dtype=tf.float32)\n",
    "        \n",
    "        # self.filter_med = tf.constant([[0,  0,  0,  0, 0],\n",
    "        #                                [0, -1,  2, -1, 0],\n",
    "        #                                [0,  2, -4,  2, 0],\n",
    "        #                                [0, -1,  2, -1, 0],\n",
    "        #                                [0,  0,  0,  0, 0]], dtype=tf.float32)\n",
    "        \n",
    "        # self.filter_large = tf.constant([[-1,  2,  -2,  2, -1],\n",
    "        #                                  [ 2, -6,   8, -6,  2],\n",
    "        #                                  [-2,  8, -12,  8, -2],\n",
    "        #                                  [ 2, -6,   8, -6,  2],\n",
    "        #                                  [-1,  2,  -2,  2, -1]], dtype=tf.float32)\n",
    "        \n",
    "        self.q_small = self.add_weight(name = 'q_small',\n",
    "                                       shape = (1, ), \n",
    "                                       initializer = tf.initializers.Constant(value=2.0),\n",
    "                                       trainable = True)\n",
    "        \n",
    "        # self.q_med = self.add_weight(name = 'q_med', \n",
    "        #                              shape = (1, ),\n",
    "        #                              initializer = tf.initializers.Constant(value=4.0),\n",
    "        #                              trainable = True)\n",
    "        \n",
    "        # self.q_large = self.add_weight(name = 'q_large', \n",
    "        #                                shape = (1, ),\n",
    "        #                                initializer = tf.initializers.Constant(value=12.0),\n",
    "        #                                trainable = True)\n",
    "        \n",
    "        self.filter_small = tf.expand_dims(tf.expand_dims(self.filter_small, axis=-1), axis=-1)\n",
    "        # self.filter_med   = tf.expand_dims(tf.expand_dims(self.filter_med,   axis=-1), axis=-1)\n",
    "        # self.filter_large = tf.expand_dims(tf.expand_dims(self.filter_large, axis=-1), axis=-1)\n",
    "\n",
    "        self.filter_small = tf.tile(self.filter_small, [1, 1, 3, 1])\n",
    "        # self.filter_med   = tf.tile(self.filter_med,   [1, 1, 3, 1])\n",
    "        # self.filter_large = tf.tile(self.filter_large, [1, 1, 3, 1])\n",
    "\n",
    "        def call(self, inputs):\n",
    "            filter_small = tf.math.divide(self.filter_small, self.q_small)\n",
    "            # filter_med   = tf.divide(self.filter_med, self.q_med)\n",
    "            # filter_large = tf.divide(self.filter_large, self.q_large)\n",
    "\n",
    "            output_small = tf.nn.conv2d(inputs, filter_small, strides=[1, 1, 1, 1], padding='SAME')\n",
    "            # output_med   = tf.nn.conv2d(inputs, filter_med,   strides=[1, 1, 1, 1], padding='SAME')\n",
    "            # output_large = tf.nn.conv2d(inputs, filter_large, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "            return output_small\n",
    "            # return tf.concat([output_small, output_med, output_large], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " psrm_4 (PSRM)               (None, 256, 256, 3)       1         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 196608)            0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 196609    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 196,610\n",
      "Trainable params: 196,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tmp_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(256, 256, 3)),\n",
    "    PSRM(),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "tmp_model.compile(optimizer = keras.optimizers.Adam(),\n",
    "                  loss = keras.losses.BinaryCrossentropy(),\n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "tmp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['q_small:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['q_small:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "138/138 [==============================] - 7s 45ms/step - loss: 1453.7006 - accuracy: 0.5764 - val_loss: 596.7103 - val_accuracy: 0.4455\n"
     ]
    }
   ],
   "source": [
    "tmp_history = tmp_model.fit(train_dataset_cdfv1, epochs=1, validation_data=val_dataset_cdfv1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Custom Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TMPDense(keras.layers.Layer):\n",
    "    def __init__(self, units, input_dim):\n",
    "        super(TMPDense, self).__init__()\n",
    "\n",
    "        self.w = self.add_weight(name = 'w',\n",
    "                                 shape = (input_dim, units),\n",
    "                                 initializer = tf.initializers.random_normal,\n",
    "                                 trainable = True)\n",
    "        \n",
    "        self.b = self.add_weight(name = 'b',\n",
    "                                  shape = (units, ),\n",
    "                                  initializer = tf._initializers.zeros,\n",
    "                                  trainable = True)\n",
    "        \n",
    "        self.m = self.add_weight(name = 'm',\n",
    "                                 shape = (1, ),\n",
    "                                 initializer = tf.initializers.Constant(value=2.0),\n",
    "                                 trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        div = tf.math.divide(self.w, self.m)\n",
    "        return tf.matmul(inputs, div) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ssrm_1 (SSRM)               (None, 256, 256, 3)       0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 196608)            0         \n",
      "                                                                 \n",
      " tmp_dense_2 (TMPDense)      (None, 5)                 983046    \n",
      "                                                                 \n",
      " tmp_dense_3 (TMPDense)      (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 983,053\n",
      "Trainable params: 983,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dModel = keras.Sequential([\n",
    "    keras.layers.Input(shape=(256,256,3)),\n",
    "    SSRM(),\n",
    "    Flatten(),\n",
    "    TMPDense(5, 196608),\n",
    "    TMPDense(1, 5),\n",
    "])\n",
    "\n",
    "dModel.compile(optimizer = keras.optimizers.Adam(),\n",
    "               loss = keras.losses.BinaryCrossentropy(),\n",
    "               metrics = ['accuracy']\n",
    "               )\n",
    "\n",
    "dModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00287271],\n",
       "        [-0.04635007],\n",
       "        [ 0.03803451],\n",
       "        [-0.02399216],\n",
       "        [-0.03421838]], dtype=float32),\n",
       " array([0.], dtype=float32),\n",
       " array([2.], dtype=float32)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dModel.layers[3].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "138/138 [==============================] - 23s 131ms/step - loss: 5.1639 - accuracy: 0.6650 - val_loss: 5.1884 - val_accuracy: 0.6636\n",
      "Epoch 2/5\n",
      "138/138 [==============================] - 10s 69ms/step - loss: 5.1708 - accuracy: 0.6648 - val_loss: 5.1884 - val_accuracy: 0.6636\n",
      "Epoch 3/5\n",
      "138/138 [==============================] - 16s 113ms/step - loss: 5.1708 - accuracy: 0.6648 - val_loss: 5.1884 - val_accuracy: 0.6636\n",
      "Epoch 4/5\n",
      "138/138 [==============================] - 12s 82ms/step - loss: 5.1708 - accuracy: 0.6648 - val_loss: 5.1884 - val_accuracy: 0.6636\n",
      "Epoch 5/5\n",
      "138/138 [==============================] - 11s 76ms/step - loss: 5.1708 - accuracy: 0.6648 - val_loss: 5.1884 - val_accuracy: 0.6636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c33ee7e920>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dModel.fit(train_dataset_cdfv1, epochs=5, validation_data=val_dataset_cdfv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.01285018],\n",
       "        [-0.05632751],\n",
       "        [ 0.04801176],\n",
       "        [-0.01401469],\n",
       "        [-0.0242409 ]], dtype=float32),\n",
       " array([-0.00997706], dtype=float32),\n",
       " array([2.0099752], dtype=float32)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dModel.layers[3].get_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Conv2D Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TMPConv(Layer):\n",
    "    def __init__(self):\n",
    "        super(TMPConv, self).__init__()\n",
    "\n",
    "        self.kernel = self.add_weight(name = 'kernel', \n",
    "                                      shape = (3, 3, 3, 1),\n",
    "                                      initializer = tf.initializers.random_normal,\n",
    "                                      trainable = True)\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.conv2d(inputs, self.kernel, strides = [1,1,1,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tmp_conv_2 (TMPConv)        (None, 256, 256, 1)       27        \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65537     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,564\n",
      "Trainable params: 65,564\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tmp_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(256, 256, 3)),\n",
    "    TMPConv(),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid'),\n",
    "])\n",
    "\n",
    "tmp_model.compile(loss=keras.losses.BinaryCrossentropy(),\n",
    "                  optimizer=Adam(),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "tmp_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 0.01082886],\n",
       "          [ 0.0721704 ],\n",
       "          [ 0.06314751]],\n",
       " \n",
       "         [[ 0.00422159],\n",
       "          [ 0.06482067],\n",
       "          [-0.12442797]],\n",
       " \n",
       "         [[ 0.01848338],\n",
       "          [ 0.00169939],\n",
       "          [-0.09155348]]],\n",
       " \n",
       " \n",
       "        [[[ 0.00671694],\n",
       "          [-0.01156982],\n",
       "          [ 0.05192506]],\n",
       " \n",
       "         [[-0.01723207],\n",
       "          [-0.00908884],\n",
       "          [ 0.04879386]],\n",
       " \n",
       "         [[-0.06806997],\n",
       "          [ 0.01221262],\n",
       "          [ 0.01048027]]],\n",
       " \n",
       " \n",
       "        [[[ 0.03121798],\n",
       "          [-0.09013098],\n",
       "          [-0.02875078]],\n",
       " \n",
       "         [[-0.02750469],\n",
       "          [ 0.04947739],\n",
       "          [-0.03677714]],\n",
       " \n",
       "         [[-0.0318876 ],\n",
       "          [ 0.02067011],\n",
       "          [ 0.07242   ]]]], dtype=float32)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 29s 203ms/step - loss: 6.1918 - acc: 0.6027 - val_loss: 1.0617 - val_acc: 0.6336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3431893f0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.fit(train_dataset_cdfv1, epochs=1, validation_data=val_dataset_cdfv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[[[ 0.01429241],\n",
       "          [ 0.07048156],\n",
       "          [ 0.06183848]],\n",
       " \n",
       "         [[ 0.0079709 ],\n",
       "          [ 0.06347314],\n",
       "          [-0.12531525]],\n",
       " \n",
       "         [[ 0.02252773],\n",
       "          [ 0.00068383],\n",
       "          [-0.09209473]]],\n",
       " \n",
       " \n",
       "        [[[ 0.01043839],\n",
       "          [-0.01300415],\n",
       "          [ 0.05080117]],\n",
       " \n",
       "         [[-0.01322258],\n",
       "          [-0.01017844],\n",
       "          [ 0.04809758]],\n",
       " \n",
       "         [[-0.0637646 ],\n",
       "          [ 0.01145741],\n",
       "          [ 0.01013192]]],\n",
       " \n",
       " \n",
       "        [[[ 0.0351655 ],\n",
       "          [-0.09134953],\n",
       "          [-0.02964653]],\n",
       " \n",
       "         [[-0.02326937],\n",
       "          [ 0.04860467],\n",
       "          [-0.03724435]],\n",
       " \n",
       "         [[-0.02735478],\n",
       "          [ 0.02013623],\n",
       "          [ 0.0723052 ]]]], dtype=float32)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model.layers[0].get_weights()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSRM(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TSRM, self).__init__(**kwargs)\n",
    "\n",
    "        self.filter_small = tf.constant([[0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 1, -2, 1, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0]], dtype=tf.float32)\n",
    "        \n",
    "        self.q_small = self.add_weight(shape=(),\n",
    "                                       initializer='ones',\n",
    "                                       dtype=tf.float32,\n",
    "                                       trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        filter_small = self.filter_small / self.q_small\n",
    "\n",
    "        # Input shape: [batch_size, height, width, channels]\n",
    "        # Filter shape: [height, width, in_channels, out_channels]\n",
    "\n",
    "        conv_small = tf.nn.conv2d(inputs, \n",
    "                                  tf.expand_dims(tf.expand_dims(filter_small, axis=-1), axis=-1), \n",
    "                                  strides=[1, 1, 1, 1], \n",
    "                                  padding='SAME')\n",
    "        \n",
    "        # conv_meddd = tf.nn.conv2d(inputs, filter_small, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "        # outputs = tf.concat([conv_small, conv_meddd], axis=-1)\n",
    "        return conv_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(256, 256, 3)),\n",
    "    TSRM(),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "tmp_model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138/138 [==============================] - 21s 133ms/step - loss: 722.0818 - accuracy: 0.5884 - val_loss: 549.3903 - val_accuracy: 0.4255\n"
     ]
    }
   ],
   "source": [
    "tmp_history = tmp_model.fit(train_dataset_cdfv1, epochs=1, validation_data=val_dataset_cdfv1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_kernel1 = np.array([[0, 0,  0, 0, 0],\n",
    "                    [0, 0,  0, 0, 0],\n",
    "                    [0, 1, -2, 1, 0],\n",
    "                    [0, 0,  0, 0, 0],\n",
    "                    [0, 0,  0, 0, 0]], dtype=float)\n",
    "\n",
    "tmp_kernel2 = np.array([[0,  0,  0,  0, 0],\n",
    "                    [0, -1,  2, -1, 0],\n",
    "                    [0,  2, -4,  2, 0],\n",
    "                    [0, -1,  2, -1, 0],\n",
    "                    [0,  0,  0,  0, 0]], dtype=float)\n",
    "\n",
    "tmp_kernel3 = np.array([[-1,  2,  -2,  2, -1],\n",
    "                    [ 2, -6,   8, -6,  2],\n",
    "                    [-2,  8, -12,  8, -2],\n",
    "                    [ 2, -6,   8, -6,  2],\n",
    "                    [-1,  2,  -2,  2, -1]], dtype=float)\n",
    "\n",
    "tmp_fixed_kernals = [tmp_kernel1, tmp_kernel2, tmp_kernel3]\n",
    "tmp_custom_layer = SRMLayer(filters=3, kernel_size=(5, 5), fixed_filters=tmp_fixed_kernals)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
