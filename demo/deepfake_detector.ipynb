{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import av\n",
    "import os\n",
    "import face_recognition\n",
    "from PIL import Image\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "import numpy as np\n",
    "import shutil\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPool2D, Flatten, Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = 'input_files/'\n",
    "INTERMEDIARY_DIR = 'intermediary_files/'\n",
    "OUTPUT_DIR = 'output_files/'\n",
    "\n",
    "TOP_DIRS = [INPUT_DIR, INTERMEDIARY_DIR, OUTPUT_DIR]\n",
    "\n",
    "IFRAME_DIR = 'iframes/'\n",
    "FACECROP_DIR = 'faces/'\n",
    "RESIDUAL_DIR = 'residual/'\n",
    "FACEIM_DIR = 'face_images/'\n",
    "RESIM_DIR = 'residual_images/'\n",
    "\n",
    "PREPROCESS_DIRS = [IFRAME_DIR, FACECROP_DIR, RESIDUAL_DIR]\n",
    "\n",
    "MESONET_PATH  = 'saved_models/mesonet/'\n",
    "SRM_PATH      = 'saved_models/srm/'\n",
    "TEMPORAL_PATH = 'saved_models/temporal/'\n",
    "SVM_PATH = 'saved_models/svm/'\n",
    "\n",
    "MODEL_DIRS = [MESONET_PATH, SRM_PATH, TEMPORAL_PATH, SVM_PATH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in TOP_DIRS:\n",
    "    try:\n",
    "        os.makedirs(dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "for dir in PREPROCESS_DIRS:\n",
    "    try:\n",
    "        os.makedirs(INTERMEDIARY_DIR + dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "for dir in MODEL_DIRS:\n",
    "    try:\n",
    "        os.makedirs(dir)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow\n",
    "\n",
    "- Read all videos present in input_files folder\n",
    "- For each video in the input directory\n",
    "    - Extract I-Frames and crop faces\n",
    "    - Extract Extract residuals\n",
    "    - save face-cropped video and residuals video\n",
    "    - In Frame-level stream\n",
    "        - Extract all frames in face-cropped video\n",
    "        - Take average of prediction results as video score\n",
    "    - In SRM stream\n",
    "        - Extract snippets from face-cropped video\n",
    "        - Take average of prediction results as video score\n",
    "    - In Temporal stream\n",
    "        - Extract all residuals from residual video\n",
    "        - Take average of prediction per segment\n",
    "        - Select the most extreme value as video score (closest to 0 or 1)\n",
    "    - In score aggregation\n",
    "        - Take average of three scores\n",
    "        - Use voting to determine class (Use extreme value of major class as video score)\n",
    "        - Use trained svm model to predict class probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_iframes(fp):\n",
    "    input_vid = av.open(fp)\n",
    "    output_vid = av.open(INTERMEDIARY_DIR + IFRAME_DIR + os.path.split(fp)[1], 'w')\n",
    "\n",
    "    in_stream = input_vid.streams.video[0]\n",
    "    in_stream.codec_context.skip_frame = \"NONKEY\"\n",
    "\n",
    "    out_stream = output_vid.add_stream(template=in_stream)\n",
    "\n",
    "    for packet in input_vid.demux(in_stream):\n",
    "        if packet.dts is None:\n",
    "            continue\n",
    "\n",
    "        if packet.is_keyframe:\n",
    "            packet.stream = out_stream\n",
    "            output_vid.mux(packet)\n",
    "\n",
    "    input_vid.close()\n",
    "    output_vid.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames(fp, dst_dir):\n",
    "    input_vid = av.open(fp)\n",
    "    filename = os.path.split(fp)[1]\n",
    "\n",
    "    try:\n",
    "        os.makedirs(dst_dir + os.path.splitext(filename)[0] + '/')\n",
    "\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    for i, frame in enumerate(input_vid.decode(), start=1):\n",
    "        frame.to_image().save(dst_dir + os.path.splitext(filename)[0] + '/' + f'frame{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MesoNet works best with images having 256x256 dimension\n",
    "# If face location borders span a smaller distance, extend the borders\n",
    "# on either side equally to ensure 256x256 image\n",
    "\n",
    "def normalize_face_borders(low, high, max_val, req_dim):\n",
    "    diff = high - low\n",
    "    if diff >= 256:\n",
    "        return\n",
    "\n",
    "    offset = float((req_dim - diff)) / 2\n",
    "    low = max(0, low - offset)\n",
    "    high = min(max_val, high + offset)\n",
    "\n",
    "    return low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Face Location: (left, top, right, bottom)\n",
    "def modify_crop_window(face_location, height, width, req_dim):\n",
    "    left, right = normalize_face_borders(face_location[0], face_location[2], width, req_dim)\n",
    "    top, bot = normalize_face_borders(face_location[1], face_location[3], height, req_dim)\n",
    "\n",
    "    face_location = (left, top, right, bot)\n",
    "\n",
    "    return face_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cropped_faces_to_video(fp, req_dim):\n",
    "    input = av.open(fp)\n",
    "    output = av.open(INTERMEDIARY_DIR + FACECROP_DIR + os.path.split(fp)[1], 'w')\n",
    "\n",
    "    in_stream = input.streams.video[0]\n",
    "    codec_name = in_stream.codec_context.name\n",
    "\n",
    "    # output video dimension should be 256x256\n",
    "    out_stream = output.add_stream(codec_name, rate=8)\n",
    "    out_stream.width = 256\n",
    "    out_stream.height = 256\n",
    "    out_stream.pix_fmt = in_stream.codec_context.pix_fmt\n",
    "\n",
    "    for frame in input.decode(in_stream):\n",
    "        img_frame = frame.to_image()\n",
    "        nd_frame = frame.to_ndarray()\n",
    "\n",
    "        # Face location returned by face_recognition api: [(top, right, bottom, left)]\n",
    "        # Origin considered at top left corner of image => right margin > left margin, bottom > top\n",
    "        face_location = face_recognition.api.face_locations(nd_frame)\n",
    "\n",
    "        # if can't find a face, then skip that frame\n",
    "        # TODO : sync frame skipping with temporality stream\n",
    "        if len(face_location) == 0:\n",
    "            continue\n",
    "\n",
    "        # Face location required by PIL.Image: (left, top, right, bottom)\n",
    "        face_location = (face_location[0][3], face_location[0][0], \n",
    "                         face_location[0][1], face_location[0][2])\n",
    "            \n",
    "        # Modify crop window size only if positive value given.\n",
    "        if (req_dim > 0):    \n",
    "            face_location = modify_crop_window(face_location, img_frame.height, img_frame.width, req_dim)\n",
    "            \n",
    "        img_frame = img_frame.crop(face_location)\n",
    "        \n",
    "        out_frame = av.VideoFrame.from_image(img_frame)\n",
    "        out_packet = out_stream.encode(out_frame)\n",
    "        output.mux(out_packet)\n",
    "\n",
    "    out_packet = out_stream.encode(None)\n",
    "    output.mux(out_packet)\n",
    "\n",
    "    input.close()\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residual(a, b):\n",
    "    return Image.fromarray(np.asarray(a) - np.asarray(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_residuals(fp):\n",
    "    input_vid = av.open(fp)\n",
    "    output_vid = av.open(INTERMEDIARY_DIR + RESIDUAL_DIR + os.path.split(fp)[1], 'w')\n",
    "\n",
    "    in_stream = input_vid.streams.video[0]\n",
    "    codec_name = in_stream.codec_context.name\n",
    "\n",
    "    # output video dimension should be 256x256\n",
    "    out_stream = output_vid.add_stream(codec_name, rate=8)\n",
    "    out_stream.width = 224\n",
    "    out_stream.height = 224\n",
    "    out_stream.pix_fmt = in_stream.codec_context.pix_fmt\n",
    "\n",
    "    # Extract residuals\n",
    "    frame_list = [frame for frame in input_vid.decode()]\n",
    "    \n",
    "    input_vid.seek(0)\n",
    "    iframe_index = [i for i, packet in enumerate(input_vid.demux()) if packet.is_keyframe]\n",
    "\n",
    "    residuals = []\n",
    "    gop_start_index = 0\n",
    "    for index in iframe_index:\n",
    "        if index == 0:\n",
    "            continue\n",
    "\n",
    "        residual = compute_residual(frame_list[index - 1].to_image(), frame_list[gop_start_index].to_image())\n",
    "        out_frame = av.VideoFrame.from_image(residual)\n",
    "        out_packet = out_stream.encode(out_frame)\n",
    "        output_vid.mux(out_packet)\n",
    "\n",
    "        gop_start_index = index\n",
    "\n",
    "    residual = compute_residual(frame_list[-1].to_image(), frame_list[gop_start_index].to_image())\n",
    "    out_frame = av.VideoFrame.from_image(residual)\n",
    "    out_packet = out_stream.encode(out_frame)\n",
    "    output_vid.mux(out_packet)\n",
    "\n",
    "    out_packet = out_stream.encode(None)\n",
    "    output_vid.mux(out_packet)\n",
    "\n",
    "    input_vid.close()\n",
    "    output_vid.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames_from_video(fp):\n",
    "    vid_container = av.open(fp)\n",
    "\n",
    "    frames = []\n",
    "    for frame in vid_container.decode():\n",
    "        frames.append(frame.to_image())\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the index of frames that begin a new segment (except the first segment)\n",
    "def get_segment_dividers(frame_count, num_segments):\n",
    "    segments_per_frame = math.floor(frame_count / num_segments)\n",
    "\n",
    "    return [(segments_per_frame * i) for i in range(1, num_segments) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the indices of the frames that will be randomly selected from each segment\n",
    "# Multiple snippets indices per segment can be returned by setting the num_snippets arg \n",
    "def get_snippet_indices(segment_dividers, num_snippets):\n",
    "    start_index = 0\n",
    "    num_snippets = 1 if num_snippets <= 0 else num_snippets\n",
    "\n",
    "    snippet_indices = []\n",
    "    for end_index in segment_dividers:\n",
    "\n",
    "        # Extracting multiple snippets per segment (if needed)\n",
    "        for _ in range(num_snippets):\n",
    "            snippet_indices.append(random.randint(start_index, end_index - 1))\n",
    "\n",
    "        start_index = end_index\n",
    "        \n",
    "    return snippet_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns an array of randomly selected snippets(PIL.Image) from each segment of the input video\n",
    "def extract_snippets(fp, num_segments, num_snippets):\n",
    "    vid_container = av.open(fp)\n",
    "    vid_stream = vid_container.streams.video[0]\n",
    "    frame_count = vid_stream.frames\n",
    "\n",
    "    snippets = []\n",
    "\n",
    "    # If number of frames in video is less than the number of frames that need to sampled\n",
    "    # then take all frames in the video\n",
    "    if frame_count < num_segments * num_snippets:\n",
    "        for frame in vid_container.decode():\n",
    "            snippets.append(frame.to_image())\n",
    "\n",
    "    else:\n",
    "        segment_dividers = get_segment_dividers(frame_count, num_segments)\n",
    "        segment_dividers = segment_dividers + [frame_count]\n",
    "\n",
    "        snippet_indices = get_snippet_indices(segment_dividers, num_snippets)\n",
    "\n",
    "        frame_index = 0\n",
    "        for frame in vid_container.decode():\n",
    "            if frame_index > max(snippet_indices):\n",
    "                break\n",
    "\n",
    "            if frame_index in snippet_indices:\n",
    "                snippets.append(frame.to_image())\n",
    "\n",
    "            frame_index += 1\n",
    "\n",
    "        \n",
    "    vid_container.close()\n",
    "    return snippets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mesonet Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_size):\n",
    "    model = keras.Sequential(name='Mesonet')\n",
    "    model.add(layers.Conv2D(input_shape=input_size, filters=8, kernel_size=3, activation='relu', padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2, 2, padding=\"same\"))\n",
    "\n",
    "    model.add(layers.Conv2D(input_shape=(128, 128, 8), filters=8, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(2, 2, padding=\"same\"))\n",
    "\n",
    "  \n",
    "    model.add(layers.Conv2D(input_shape=(64, 64, 8), filters=16, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(4, 4, padding=\"same\"))\n",
    "\n",
    "  \n",
    "    model.add(layers.Conv2D(input_shape=(16, 16, 16), filters=16, kernel_size=5, activation='relu', padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool2D(4, 4, padding=\"same\"))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(layers.Dense(16))\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "  \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (256, 256, 3)\n",
    "mesonet_model = create_model(input_size)\n",
    "mesonet_model.compile(optimizer=Adam(learning_rate=0.0001), \n",
    "              loss=keras.losses.BinaryCrossentropy(), \n",
    "              metrics = [keras.metrics.BinaryAccuracy(), \n",
    "                         keras.metrics.Precision(), \n",
    "                         keras.metrics.Recall(),\n",
    "                         keras.metrics.AUC(),\n",
    "                         keras.metrics.FalseNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.TruePositives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning only dense layers\n",
    "# Helper function uses `prune_low_magnitude` to make only the \n",
    "# Dense layers train with pruning.\n",
    "def apply_pruning_to_dense(layer):\n",
    "    \n",
    "    if isinstance(layer, keras.layers.Dense):\n",
    "        return tfmot.sparsity.keras.prune_low_magnitude(layer)\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From f:\\Extra_Software\\Python\\Python\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# Use `tf.keras.models.clone_model` to apply `apply_pruning_to_dense` \n",
    "# to the layers of the model.\n",
    "mesonet_model_dense_prune = keras.models.clone_model(\n",
    "    mesonet_model,\n",
    "    clone_function=apply_pruning_to_dense,\n",
    ")\n",
    "\n",
    "mesonet_model_dense_prune.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    metrics = [keras.metrics.BinaryAccuracy(), \n",
    "                         keras.metrics.Precision(), \n",
    "                         keras.metrics.Recall(),\n",
    "                         keras.metrics.AUC(),\n",
    "                         keras.metrics.FalseNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.TruePositives()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2244b1eb8e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mesonet_model_dense_prune.load_weights(MESONET_PATH + 'model_pruned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mesonet_score(model, fp):\n",
    "    frames = extract_frames_from_video(fp)\n",
    "\n",
    "    tf_frames = []\n",
    "    for frame in frames:\n",
    "        tf_frames.append(tf.convert_to_tensor(frame))\n",
    "\n",
    "    tf_frames = tf.convert_to_tensor(tf_frames)\n",
    "    results = model.predict(tf_frames, verbose=0)\n",
    "\n",
    "    return np.average(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SRM Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRMLayer(keras.layers.Layer):\n",
    "    def __init__(self, strides=[1,1,1,1], padding='SAME'):\n",
    "        super(SRMLayer, self).__init__()\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "\n",
    "        # Set of 3 fixed SRM Filters used to extract noise & semantic features\n",
    "        self.filter_small = tf.constant([[0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 1, -2, 1, 0],\n",
    "                                         [0, 0,  0, 0, 0],\n",
    "                                         [0, 0,  0, 0, 0]], dtype=tf.float32)\n",
    "        \n",
    "        self.filter_med = tf.constant([[0,  0,  0,  0, 0],\n",
    "                                       [0, -1,  2, -1, 0],\n",
    "                                       [0,  2, -4,  2, 0],\n",
    "                                       [0, -1,  2, -1, 0],\n",
    "                                       [0,  0,  0,  0, 0]], dtype=tf.float32)\n",
    "        \n",
    "        self.filter_large = tf.constant([[-1,  2,  -2,  2, -1],\n",
    "                                         [ 2, -6,   8, -6,  2],\n",
    "                                         [-2,  8, -12,  8, -2],\n",
    "                                         [ 2, -6,   8, -6,  2],\n",
    "                                         [-1,  2,  -2,  2, -1]], dtype=tf.float32)\n",
    "\n",
    "        # Learnability in SRM filters introduced by 'q' values\n",
    "        # SRM filters are divided by their respective 'q' values before convolution\n",
    "        # 'q' values are updated during backpropagation using gradient descent\n",
    "        self.q_small = self.add_weight(name='q_small',\n",
    "                                       shape=(5, 5, 3, 1),\n",
    "                                       initializer=keras.initializers.Constant(value=2.0),\n",
    "                                       trainable=True)\n",
    "        \n",
    "        self.q_med = self.add_weight(name='q_med',\n",
    "                                     shape=(5, 5, 3, 1),\n",
    "                                     initializer=keras.initializers.Constant(value=4.0),\n",
    "                                     trainable=True)\n",
    "        \n",
    "        self.q_large = self.add_weight(name='q_large',\n",
    "                                       shape=(5, 5, 3, 1),\n",
    "                                       initializer=keras.initializers.Constant(value=12.0),\n",
    "                                       trainable=True)\n",
    "        \n",
    "        # 3rd dimension of filters => number of input channels (Three channels)\n",
    "        self.filter_small = tf.stack([self.filter_small, self.filter_small, self.filter_small], axis=2)\n",
    "        self.filter_med   = tf.stack([self.filter_med, self.filter_med, self.filter_med], axis=2)\n",
    "        self.filter_large = tf.stack([self.filter_large, self.filter_large, self.filter_large], axis=2)\n",
    "\n",
    "        # 4th dimension of filters => number of output feature maps (One feature map)\n",
    "        # Each filter gives a single output feature map\n",
    "        self.filter_small = tf.expand_dims(self.filter_small, axis=-1)\n",
    "        self.filter_med   = tf.expand_dims(self.filter_med, axis=-1)\n",
    "        self.filter_large = tf.expand_dims(self.filter_large, axis=-1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        filter_small = tf.math.divide(self.filter_small, self.q_small)\n",
    "        filter_med   = tf.math.divide(self.filter_med, self.q_med)\n",
    "        filter_large = tf.math.divide(self.filter_large, self.q_large)\n",
    "\n",
    "        output_small = tf.nn.conv2d(inputs, filter_small, strides=self.strides, padding=self.padding)\n",
    "        output_med   = tf.nn.conv2d(inputs, filter_med,   strides=self.strides, padding=self.padding)\n",
    "        output_large = tf.nn.conv2d(inputs, filter_large, strides=self.strides, padding=self.padding)\n",
    "\n",
    "        return tf.concat([output_small, output_med, output_large], axis=3)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(SRMLayer, self).get_config()\n",
    "        config.update({'strides': self.strides,\n",
    "                       'padding': self.padding,\n",
    "                       'filter_small': self.filter_small,\n",
    "                       'filter_med': self.filter_med,\n",
    "                       'filter_large': self.filter_large,\n",
    "                       'q_small': self.q_small,\n",
    "                       'q_med': self.q_med,\n",
    "                       'q_large': self.q_large})\n",
    "        \n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "XceptionNetwork = keras.applications.Xception(\n",
    "    include_top = False,\n",
    "    weights = 'imagenet',\n",
    "    input_shape = (256, 256, 3),\n",
    "    pooling = max,\n",
    "    classes = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_SRM_model(xception_training):\n",
    "    inputs = keras.layers.Input(shape=(256, 256, 3))\n",
    "    SRM_noise_maps = SRMLayer()(inputs)\n",
    "    \n",
    "    feature_maps = tf.keras.applications.xception.preprocess_input(SRM_noise_maps)\n",
    "    feature_maps = XceptionNetwork(feature_maps, training=xception_training)\n",
    "\n",
    "    features = keras.layers.Flatten()(feature_maps)\n",
    "    features = keras.layers.Dropout(0.8)(features)\n",
    "    features = keras.layers.Dense(units=130, activation=keras.layers.LeakyReLU())(features)\n",
    "    outputs = keras.layers.Dense(units=1, activation='sigmoid')(features)\n",
    "\n",
    "    return keras.Model(inputs, outputs, name='SRM_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRM_Model = create_SRM_model(xception_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(beta_1=0.9, \n",
    "                                  beta_2=0.999, \n",
    "                                  epsilon=1e-6, \n",
    "                                  learning_rate=0.00002)\n",
    "\n",
    "SRM_Model.compile(optimizer=optimizer,\n",
    "                      loss=keras.losses.BinaryCrossentropy(),\n",
    "                      metrics=[keras.metrics.BinaryAccuracy(), \n",
    "                               keras.metrics.Precision(), \n",
    "                               keras.metrics.Recall(),\n",
    "                               keras.metrics.AUC(),\n",
    "                               keras.metrics.FalseNegatives(),\n",
    "                               keras.metrics.FalsePositives(),\n",
    "                               keras.metrics.TrueNegatives(),\n",
    "                               keras.metrics.TruePositives()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2244b1e8eb0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRM_Model.load_weights(SRM_PATH + 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_srm_score(model, fp):\n",
    "    frames = extract_snippets(fp, num_segments=8, num_snippets=1)\n",
    "\n",
    "    tf_frames = []\n",
    "    for frame in frames:\n",
    "        tf_frames.append(tf.convert_to_tensor(frame))\n",
    "\n",
    "    tf_frames = tf.convert_to_tensor(tf_frames)\n",
    "    results = model.predict(tf_frames, verbose=0)\n",
    "\n",
    "    return np.average(results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50v2 = tf.keras.applications.ResNet50V2(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.layers.Input((224, 224, 3))\n",
    "x = tf.keras.applications.resnet_v2.preprocess_input(inputs)\n",
    "x = resnet50v2(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.8)(x)\n",
    "x = Dense(100, activation=LeakyReLU())(x)\n",
    "x = Dropout(0.8)(x)\n",
    "out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "temporal_model = keras.Model(inputs, out, name=\"temporal_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_model.compile(optimizer = Adam(learning_rate = 0.00001), \n",
    "              loss = keras.losses.BinaryCrossentropy(), \n",
    "              metrics = [keras.metrics.BinaryAccuracy(), \n",
    "                         keras.metrics.Precision(), \n",
    "                         keras.metrics.Recall(),\n",
    "                         keras.metrics.AUC(),\n",
    "                         keras.metrics.FalseNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.TruePositives()],\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x2244e948d60>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temporal_model.load_weights(TEMPORAL_PATH + 'checkpoint_final_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals(fp, num_segments):\n",
    "    vid_container = av.open(fp)\n",
    "    vid_stream = vid_container.streams.video[0]\n",
    "    frame_count = vid_stream.frames\n",
    "\n",
    "    segment_dividers = get_segment_dividers(frame_count, num_segments)\n",
    "\n",
    "    vid_container.seek(0)\n",
    "    frame_list = [frame.to_image() for frame in vid_container.decode()]\n",
    "\n",
    "    residuals = []\n",
    "    start_index = 0\n",
    "    for sd in segment_dividers:\n",
    "        residuals.append(frame_list[start_index:sd])\n",
    "        start_index = sd\n",
    "    \n",
    "    residuals.append(frame_list[start_index:])\n",
    "\n",
    "    vid_container.close()\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_temporal_score(model, fp, num_segments):\n",
    "    residuals = get_residuals(fp, num_segments=num_segments)\n",
    "\n",
    "    results = []\n",
    "    for residual_set in residuals:\n",
    "        tf_frames = []\n",
    "\n",
    "        for frame in residual_set:\n",
    "            tf_frames.append(img_to_array(tf.image.resize(frame, size = [224, 224])))\n",
    "\n",
    "        tf_frames = np.asarray(tf_frames)\n",
    "        result = model.predict(tf_frames, verbose=0)\n",
    "\n",
    "        real_scores = [score for score in result if score >= 0.5]\n",
    "        fake_scores = [score for score in result if score < 0.5]\n",
    "\n",
    "        seg_avg = np.average(real_scores) if len(real_scores) > len(fake_scores) else np.average(fake_scores)\n",
    "        results.append(seg_avg)\n",
    "\n",
    "    real_seg_scores = [score for score in results if score >= 0.5]\n",
    "    fake_seg_scores = [score for score in results if score < 0.5]\n",
    "\n",
    "    return np.average(real_seg_scores) if len(real_seg_scores) > len(fake_seg_scores) else np.average(fake_seg_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SVM_PATH + 'svm.pkl', 'rb') as f:\n",
    "    svm_model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class(score):\n",
    "    return 'Fake' if score < 0.5 else 'Real'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_score(m, s, t, w1=0.33, w2=0.33, w3=0.33):\n",
    "    avg = (m * w1) + (s * w2) + (t * w3)\n",
    "\n",
    "    return get_class(avg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_voting_score(m, s, t):\n",
    "    fake_pred_count = sum(1 if score < 0.5 else 0 for score in [m, s, t])\n",
    "\n",
    "    score = max([m, s, t]) if fake_pred_count < 2 else min([m, s, t])\n",
    "\n",
    "    return get_class(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifer_score(m, s, t):\n",
    "    prediction = svm_model.predict_proba([[m, s, t]])\n",
    "\n",
    "    return get_class(prediction[0][1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_result_report(results):\n",
    "    final_table = PrettyTable()\n",
    "    final_table.field_names = [\"File Name\", \"Weighted Avg Result\", \"Majority Voting Result\", \"SVM Classfier Result\"]\n",
    "\n",
    "    final_table.add_rows(results)\n",
    "    print(final_table)\n",
    "\n",
    "    now = datetime.now()\n",
    "\n",
    "    with open(OUTPUT_DIR + now.strftime(\"%Y%m%d_%H%M%S\") + '.txt', 'w') as f:\n",
    "        f.write('SCORE AGGREGATION\\n')\n",
    "        f.write(final_table.get_string())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(fp):\n",
    "    filename = os.path.split(fp)[1]\n",
    "\n",
    "    if not os.path.exists(INTERMEDIARY_DIR + IFRAME_DIR + filename):\n",
    "        extract_iframes(fp)\n",
    "\n",
    "    if not os.path.exists(INTERMEDIARY_DIR + FACECROP_DIR + filename):\n",
    "        save_cropped_faces_to_video(INTERMEDIARY_DIR + IFRAME_DIR + filename, -1)\n",
    "\n",
    "    if not os.path.exists(INTERMEDIARY_DIR + RESIDUAL_DIR + filename):\n",
    "        extract_residuals(fp)\n",
    "\n",
    "    m_score = calculate_mesonet_score(mesonet_model_dense_prune, INTERMEDIARY_DIR + FACECROP_DIR + filename)\n",
    "    s_score = calculate_srm_score(SRM_Model, INTERMEDIARY_DIR + FACECROP_DIR + filename)\n",
    "    t_score = calculate_temporal_score(temporal_model, INTERMEDIARY_DIR + RESIDUAL_DIR + filename, 3)\n",
    "\n",
    "    wa_result = weighted_avg_score(m_score, s_score, t_score)\n",
    "    mv_result = majority_voting_score(m_score, s_score, t_score)\n",
    "    svm_result = svm_classifer_score(m_score, s_score, t_score)\n",
    "\n",
    "    return (filename, wa_result, mv_result, svm_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processed: ak1_f.mp4\n",
      "Video processed: ak1_r.mp4\n",
      "Video processed: ak2_f.mp4\n",
      "Video processed: ak2_r.mp4\n",
      "Video processed: bradd_f.mp4\n",
      "Video processed: bradd_r.mp4\n",
      "Video processed: emma_f.mp4\n",
      "Video processed: emma_r.mp4\n",
      "Video processed: jen_f.mp4\n",
      "Video processed: jen_r.mp4\n",
      "+-------------+---------------------+------------------------+----------------------+\n",
      "|  File Name  | Weighted Avg Result | Majority Voting Result | SVM Classfier Result |\n",
      "+-------------+---------------------+------------------------+----------------------+\n",
      "|  ak1_f.mp4  |         Fake        |          Fake          |         Fake         |\n",
      "|  ak1_r.mp4  |         Real        |          Real          |         Real         |\n",
      "|  ak2_f.mp4  |         Fake        |          Fake          |         Fake         |\n",
      "|  ak2_r.mp4  |         Real        |          Real          |         Real         |\n",
      "| bradd_f.mp4 |         Fake        |          Fake          |         Fake         |\n",
      "| bradd_r.mp4 |         Fake        |          Fake          |         Fake         |\n",
      "|  emma_f.mp4 |         Fake        |          Fake          |         Fake         |\n",
      "|  emma_r.mp4 |         Real        |          Real          |         Real         |\n",
      "|  jen_f.mp4  |         Fake        |          Fake          |         Fake         |\n",
      "|  jen_r.mp4  |         Real        |          Real          |         Real         |\n",
      "+-------------+---------------------+------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "filename='temp_r2.mp4'\n",
    "filename=None\n",
    "\n",
    "results = []\n",
    "\n",
    "# If no filename was given, process all videos in input directory\n",
    "if filename == None or not os.path.exists(filename):\n",
    "    for video in os.listdir(INPUT_DIR):\n",
    "        result = process_video(INPUT_DIR + video)\n",
    "        results.append(result)\n",
    "\n",
    "        print(f'Video processed: {video}')\n",
    "\n",
    "# If filename is a valid file in root directory, process only that file\n",
    "else:\n",
    "    result = process_video(filename)\n",
    "    results.append(result)\n",
    "\n",
    "generate_result_report(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in os.listdir(INTERMEDIARY_DIR + FACECROP_DIR):\n",
    "    fp = INTERMEDIARY_DIR + FACECROP_DIR + video\n",
    "\n",
    "    save_frames(fp, INTERMEDIARY_DIR + FACEIM_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in os.listdir(INTERMEDIARY_DIR + RESIDUAL_DIR):\n",
    "    fp = INTERMEDIARY_DIR + RESIDUAL_DIR + video\n",
    "\n",
    "    save_frames(fp, INTERMEDIARY_DIR + RESIM_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
